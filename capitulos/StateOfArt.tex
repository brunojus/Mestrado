\chapter{State of the art} \label{capitulo2}

The authors from the paper \cite{Lategahn2013} have presented the next-generation driver assistance systems require precise self-localization. The accuracy of the system is evaluated by computing two independent ego positions of the same trajectory from two distinct cameras and investigating these measures for consistency.

The authors of  \cite{Sankaranarayanan2008} said the video cameras are among the most commonly used sensors in a large number of applications, ranging from surveillance to smart rooms for videoconferencing. In this regard, this paper's primary focus is to highlight the efficient use of the geometric constraints induced by the imaging devices to derive distributed algorithms for target detection, tracking, and recognition.


In \cite{Unlu2019}, the authors said that the drone had seen a tremendous increase in the last few years, making these devices highly accessible to the public. Moreover, Computer vision is extensively used to detect drones autonomously compared to other proposed solutions such as RADAR, acoustics, and RF signal analysis thanks to its robustness. The authors aimed to combine a multi-frame deep learning detection technique, where the frame coming from the zoomed camera on the turret is overlaid on the wide-angle static camera's frame.


In the paper, \cite{Pei2019} was developed three-dimensional coordinates of objects captured by a sequence of images taken in different views. Object reconstruction is a technique that aims to recover the shape and appearance of objects. A novel method to reconstruct occluded objects based on synthetic aperture imaging. The proposed method labels occlusion pixels according to variance and reconstructs the 3D point cloud based on synthetic aperture imaging.


In \cite{Zaarane2020}, the authors proposed an inter-vehicle distance measurement system for self-driving based on image processing. The proposed method uses two cameras mounted as one stereo camera in the hosting vehicle behind the rear-view mirror.  Extensive experiments have shown the high accuracy of the proposed method compared to the previous works from the literature. Also, this method could be used in several systems of various domains in real-time regardless of the object types.

This paper \cite{Wu2019} presented a multi-camera vehicle detection system that significantly improves the detection performance under occlusion conditions. They also inferred the vehicle position on the ground plane by leveraging a multi-view cross-camera context.

In paper \cite{Ali2016}, the authors proposed two approaches: The  Estimation of the distance using an onboard camera and car position detection and vehicle position detection is a method of specifying the relative position to the road can serve as Lane Departure Warning system. 


The authors of \cite{Hane2017} used a surround multi-camera system to cover the full 360-degree field-of-view around the car. Their pipeline can precisely calibrate multi-camera systems, build sparse 3D maps for visual navigation, visually localize the vehicle for these maps, generate accurate dense graphs, and detect obstacles based on real-time depth map extraction.

Unlike existing methods that use pinhole cameras, the implementation of \cite{Cui2019} is based on fisheye cameras whose large field of view benefits various computer vision applications for self-driving vehicles visual-inertial odometry, visual localization, and object detection. To maintain both accuracy and efficiency, while accounting for the fact that fisheye images have a lower angular resolution, we recover the depths using multiple image resolutions. At the end of the pipeline, we fuse the fisheye depth images into the truncated signed distance function volume to obtain a 3D map.

In \cite{Huang2019}, inter-vehicle distance estimation from an in-car camera based on monocular vision is critical.  An improved method for estimating a monocular vision vehicle's distance based on the detection and segmentation of the target vehicle is proposed in this paper to address the vehicle attitude angle problem. The angle regression model is used to obtain the attitude angle information of the target vehicle. The dimension estimation network determines the actual dimensions of the target vehicle.

In \cite{Bao2016}, the multipath and non-line-of-sight effects to GPS receiver decrease the precision of the vehicle's self-localization. More specifically, the lateral error is more severe because of the blockage of the satellites. The lateral distance between building and vehicle estimated by a stereo camera is compared with a 3D building map to rectify the vehicle's lateral position. Besides, this paper employs an inertial sensor and GPS receiver to decide the longitudinal location of the car.

In \cite{Tang2019} introduces CityFlow, a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across ten intersections, with the longest distance between two simultaneous cameras being 2.5 km. We expect this dataset to catalyze research in this field, propel the state-of-the-art forward, and lead to deployed traffic optimization in the real world.

In \cite{Qi2019}, the authors measure the distance between the ego-vehicle and the target vehicle by using a monocular vision. They also eliminate estimation error by changing the vehicle pose, proposing a distance estimation method based on the car pose information. The proposed technique can be used to reduce the possible failure of distance estimation produced by changing the inclination angle and roll angle of an unmanned vehicle. Furthermore, the pose information could also help evaluate distance if the car is on a slope.

In paper \cite{Wongsaree2018} presents a distance determination technique using an image from the single forward camera. Therefore, automatic brightness adjustment and inverse perspective mapping are applied in the proposed scheme. The experimental results confirm that the proposed technique can detect the object's distance in front of the car, where the error is 7.96\%.

The paper \cite{Pan2019}, the authors simulated experiments to verify the feasibility of the proposed method. Meanwhile, physical experiment results show that this method can reduce the outdoor environment impact and improve the calibration and measurement precision effectively. Furthermore, In paper \cite{Lin2014} presents a novel method of camera parameters calibration for obstacle distance measurement based on monocular vision. In the end, the experiment shows that the proposed method is advantageous.

In \cite{Simon2019a} the authors have performed experiments on the KITTI dataset for accurate 3D object detection, and they achieved the same results as state-of-the-art in all related categories while maintaining the performance and accuracy trade-off and still run in real-time.


In this paper, we propose weighted-mean YOLO to improve object detection's real-time performance by fusing information of RGB camera and LIDAR. We designed the system using weighted-mean to construct a robust network and compared it with other algorithms. It shows performance improvement of missed-detection \cite{Kim2019}. In paper \cite{Simon2019} introduced the Complex-YOLO, a state of the art real-time 3D object detection network on point clouds only. In this work, they described a system that expands YOLOv2, a fast 2D standard object detector for RGB images, by a specific complex regression strategy to estimate multi-class 3D boxes in Cartesian space. 


In \cite{Oliveira2015}, the method consists of mapping images to a new coordinate system where perspective effects are removed. The removal of perspective associated effects facilitates road and obstacle detection and also assists in free space estimation. As shown in the results, this considerably improves the algorithm's effectiveness and reduces computation time compared with the classical inverse perspective mapping.

The approach introduced by \cite{Salman2017}, a stereo camera, was used, and the calculation of distance considers angular distance, the distance between cameras, and the pixel of the image. They proposed a method that measures object distance based on trigonometry.   

 In \cite{Rangesh2019}, a new framework for tracking multiple objects is presented. They used fusion techniques to achieve this. Second, the purposes of interest are followed directly in the real world. They tested it on real-world highway data collected from a massively sensorized testbed capable of capturing full-surround information.



This paper proposes an intelligent transport system positioning technique that determines the distance between vehicles via image sensor-based visible light communication. A new novel method determines the distance between two cars using a low camera resolution \cite{Tram2018}.


The object detection, classification, and localization in the real world scenario are studied and discussed by \cite{Hofmann2019}. Furthermore, with the opposition, the suggested approach fuses three different algorithms for object detection and classification and uses stereo vision for object localization.  An algorithm for merging the results of the three object detection methods based on bounding boxes is introduced. The proposed fusion algorithm for bounding boxes improves the detection results and provides an information fusion.

