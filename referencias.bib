@article{Lategahn2013,
abstract = {Next generation driver assistance systems require precise self localization. Common approaches using global navigation satellite systems (GNSSs) suffer from multipath and shadowing effects often rendering this solution insufficient. In urban environments this problem becomes even more pronounced. Herein we present a system for six degrees of freedom (DOF) ego localization using a mono camera and an inertial measurement unit (IMU). The camera image is processed to yield a rough position estimate using a previously computed landmark map. Thereafter IMU measurements are fused with the position estimate for a refined localization update. Moreover, we present the mapping pipeline required for the creation of landmark maps. Finally, we present experiments on real world data. The accuracy of the system is evaluated by computing two independent ego positions of the same trajectory from two distinct cameras and investigating these estimates for consistency. A mean localization accuracy of 10 cm is achieved on a 10 km sequence in an inner city scenario. {\textcopyright} 2013 IEEE.},
author = {Lategahn, Henning and Schreiber, Markus and Ziegler, Julius and Stiller, Christoph},
doi = {10.1109/IVS.2013.6629552},
file = {:home/bruno/Documentos/Mestrado/LategahnSchreiberZieglerStiller2013iv.pdf:pdf},
isbn = {9781467327558},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {June},
pages = {719--724},
title = {{Urban localization with camera and inertial measurement unit}},
year = {2013}
}
@article{Hofmann2019,
abstract = {In the future, autonomously driving vehicles have to navigate in challenging environments. In some situations, their perception capabilities are not able to generate a reliable overview of the environment, by reason of occlusions. In this contribution, an infrastructural stereo camera system for environment perception is proposed. Similar existing systems only detect moving objects by background subtraction algorithms and monocular cameras. In contrast, the proposed approach fuses three different algorithms for object detection and classification and uses stereo vision for object localization. The algorithmic concept is composed of a background subtraction algorithm based on Gaussian Mixture Models, the convolutional neural network”You only look once” as well as a novel algorithm for detecting salient objects in depth maps. The combination of these complementary object detection principles allows the reliable detection of dynamic as well as static objects. An algorithm for fusing the results of the three object detection methods based on bounding boxes is introduced. The proposed fusion algorithm for bounding boxes improves the detection results and provides an information fusion. We evaluate the proposed concept on real word data. The object detection, classification and localization in the real world scenario is investigated and discussed.},
author = {Hofmann, Christian and Particke, Florian and Hiller, Markus and Thielecke, J{\"{o}}rn},
doi = {10.5220/0007370408080815},
file = {:home/bruno/Documentos/Mestrado/VISAPP{\_}2019{\_}104{\_}CR.pdf:pdf},
isbn = {9789897583544},
journal = {VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
keywords = {Autonomous Driving,Deep Learning,Infrastructural Cameras,Object Detection,Robotics,Stereo Vision},
pages = {808--815},
title = {{Object detection, classification and localization by infrastructural stereo cameras}},
volume = {5},
year = {2019}
}
@article{Cui2019,
abstract = {We present a real-time dense geometric mapping algorithm for large-scale environments. Unlike existing methods which use pinhole cameras, our implementation is based on fisheye cameras whose large field of view benefits various computer vision applications for self-driving vehicles such as visual-inertial odometry, visual localization, and object detection. Our algorithm runs on in-vehicle PCs at approximately 15 Hz, enabling vision-only 3D scene perception for self-driving vehicles. For each synchronized set of images captured by multiple cameras, we first compute a depth map for a reference camera using plane-sweeping stereo. To maintain both accuracy and efficiency, while accounting for the fact that fisheye images have a lower angular resolution, we recover the depths using multiple image resolutions. We adopt the fast object detection framework, YOLOv3, to remove potentially dynamic objects. At the end of the pipeline, we fuse the fisheye depth images into the truncated signed distance function (TSDF) volume to obtain a 3D map. We evaluate our method on large-scale urban datasets, and results show that our method works well in complex dynamic environments.},
archivePrefix = {arXiv},
arxivId = {1809.06132},
author = {Cui, Zhaopeng and Heng, Lionel and Yeo, Ye Chuan and Geiger, Andreas and Pollefeys, Marc and Sattler, Torsten},
doi = {10.1109/ICRA.2019.8793884},
eprint = {1809.06132},
file = {:home/bruno/Documentos/Mestrado/1809.06132.pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {6087--6093},
title = {{Real-time dense mapping for self-driving vehicles using fisheye cameras}},
volume = {2019-May},
year = {2019}
}
@article{Morsu,
author = {Morsu},
file = {:home/bruno/Documentos/Mestrado/morsu.pdf:pdf},
pages = {1--10},
title = {{Camera Based Moving Object Detection for Autonomous Driving}}
}
@article{Hane2017,
abstract = {Cameras are a crucial exteroceptive sensor for self-driving cars as they are low-cost and small, provide appearance information about the environment, and work in various weather conditions. They can be used for multiple purposes such as visual navigation and obstacle detection. We can use a surround multi-camera system to cover the full 360-degree field-of-view around the car. In this way, we avoid blind spots which can otherwise lead to accidents. To minimize the number of cameras needed for surround perception, we utilize fisheye cameras. Consequently, standard vision pipelines for 3D mapping, visual localization, obstacle detection, etc. need to be adapted to take full advantage of the availability of multiple cameras rather than treat each camera individually. In addition, processing of fisheye images has to be supported. In this paper, we describe the camera calibration and subsequent processing pipeline for multi-fisheye-camera systems developed as part of the V-Charge project. This project seeks to enable automated valet parking for self-driving cars. Our pipeline is able to precisely calibrate multi-camera systems, build sparse 3D maps for visual navigation, visually localize the car with respect to these maps, generate accurate dense maps, as well as detect obstacles based on real-time depth map extraction.},
archivePrefix = {arXiv},
arxivId = {1708.09839},
author = {H{\"{a}}ne, Christian and Heng, Lionel and Lee, Gim Hee and Fraundorfer, Friedrich and Furgale, Paul and Sattler, Torsten and Pollefeys, Marc},
doi = {10.1016/j.imavis.2017.07.003},
eprint = {1708.09839},
file = {:home/bruno/Documentos/Mestrado/3D{\_}Visual{\_}Perception{\_}for{\_}Self-Driving{\_}Cars{\_}using{\_}a.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Calibration,Fisheye camera,Localization,Mapping,Multi-camera system,Obstacle detection},
number = {August},
pages = {14--27},
title = {{3D visual perception for self-driving cars using a multi-camera system: Calibration, mapping, localization, and obstacle detection}},
volume = {68},
year = {2017}
}
@article{Ali2016,
abstract = {This paper describes systematically two methods used in intelligent transportation systems: Distance Estimation using an onboard camera and car position detection. Distance estimation is a method for detecting distance for the preceding vehicles based on monocular camera. Vehicle position detection is a method of specifying the vehicle position relative to the road that can serve as Lane Departure Warning system. These two approaches have been discussed and implemented in this article. For lane detection and tracking, Hough Transform and Kalman filter were adopted. A brief introduction about both lane detection system and object detection is given. Finally, both approaches have been evaluated on a large dataset of videos.},
author = {Ali, Abduladhem Abdulkareem and Hussein, Hussein Alaa},
doi = {10.1109/AIC-MITCSA.2016.7759904},
file = {:home/bruno/Documentos/Mestrado/07759904.pdf:pdf},
isbn = {9781509032471},
journal = {Al-Sadiq International Conference on Multidisciplinary in IT and Communication Techniques Science and Applications, AIC-MITCSA 2016},
keywords = {Distance estimation,driver assistance systems,lane departure warning system,onboard vehicular camera,vehicle position},
number = {1},
pages = {20--23},
publisher = {IEEE},
title = {{Distance estimation and vehicle position detection based on monocular camera}},
year = {2016}
}
@article{Qi2019,
abstract = {In order to measure the distance between our vehicle and the target vehicle by monocular vision and eliminate the estimation error bring by changing of vehicle pose, we propose the distance estimation method based on the vehicle pose information, which can be used to eliminate the error of distance estimation effectively cause by the change of the pitch angle and roll angle of unmanned vehicle. In addition, the pose information could also help us estimate whether the vehicle is in a slope, thus engage in distance estimation for the vehicles. Several groups of data was collected for the experiments. And the result prove the validity of the algorithm in distance estimation.},
author = {Qi, S. H. and Li, J. and Sun, Z. P. and Zhang, J. T. and Sun, Y.},
doi = {10.1088/1742-6596/1168/3/032040},
file = {:home/bruno/Documentos/Mestrado/Qi{\_}2019{\_}J.{\_}Phys.{\_}{\_}Conf.{\_}Ser.{\_}1168{\_}032040.pdf:pdf},
issn = {17426596},
journal = {Journal of Physics: Conference Series},
number = {3},
title = {{Distance Estimation of Monocular Based on Vehicle Pose Information}},
volume = {1168},
year = {2019}
}
@article{Tang2019,
abstract = {Urban traffic optimization using traffic cameras as sensors is driving the need to advance state-of-the-art multi-target multi-camera (MTMC) tracking. This work introduces CityFlow, a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in terms of spatial coverage and the number of cameras/videos in an urban environment. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. Camera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identification (ReID). We conducted an extensive experimental evaluation of baselines/state-of-the-art approaches in MTMC tracking, multi-target single-camera (MTSC) tracking, object detection, and image-based ReID on this dataset, analyzing the impact of different network architectures, loss functions, spatio-temporal models and their combinations on task effectiveness. An evaluation server is launched with the release of our benchmark at the 2019 AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to compare the performance of their newest techniques. We expect this dataset to catalyze research in this field, propel the state-of-the-art forward, and lead to deployed traffic optimization(s) in the real world.},
archivePrefix = {arXiv},
arxivId = {1903.09254},
author = {Tang, Zheng and Naphade, Milind and Liu, Ming Yu and Yang, Xiaodong and Birchfield, Stan and Wang, Shuo and Kumar, Ratnesh and Anastasiu, David and Hwang, Jenq Neng},
doi = {10.1109/CVPR.2019.00900},
eprint = {1903.09254},
file = {:home/bruno/Documentos/Mestrado/Tang{\_}CityFlow{\_}A{\_}City-Scale{\_}Benchmark{\_}for{\_}Multi-Target{\_}Multi-Camera{\_}Vehicle{\_}Tracking{\_}and{\_}CVPR{\_}2019{\_}paper.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Big Data,Datasets and Evaluation,Large Scale Methods,Motion and Tracking},
pages = {8789--8798},
title = {{Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification}},
volume = {2019-June},
year = {2019}
}
@article{Mino2016,
author = {Mino, Atsushi},
file = {:home/bruno/Documentos/Mestrado/42-7.pdf:pdf},
pages = {47--51},
title = {{Obstacle Detection by Monocular Camera}},
year = {2016}
}
@article{Huang2019,
abstract = {Advanced driver assistance systems (ADAS) based on monocular vision are rapidly becoming a popular research subject. In ADAS, inter-vehicle distance estimation from an in-car camera based on monocular vision is critical. At present, related methods based on a monocular vision for measuring the absolute distance of vehicles ahead experience accuracy problems in terms of the ranging result, which is low, and the deviation of the ranging result between different types of vehicles, which is large and easily affected by a change in the attitude angle. To improve the robustness of a distance estimation system, an improved method for estimating the distance of a monocular vision vehicle based on the detection and segmentation of the target vehicle is proposed in this paper to address the vehicle attitude angle problem. The angle regression model (ARN) is used to obtain the attitude angle information of the target vehicle. The dimension estimation network determines the actual dimensions of the target vehicle. Then, a 2D base vector geometric model is designed in accordance with the image analytic geometric principle to accurately recover the back area of the target vehicle. Lastly, area-distance modeling based on the principle of camera projection is performed to estimate distance. The experimental results on the real-world computer vision benchmark, KITTI, indicate that our approach achieves superior performance compared with other existing published methods for different types of vehicles (including front and sideway vehicles).},
author = {Huang, Liqin and Zhe, Ting and Wu, Junyi and Wu, Qiang and Pei, Chenhao and Chen, Dan},
doi = {10.1109/ACCESS.2019.2907984},
file = {:home/bruno/Documentos/Mestrado/08678911.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Attitude angle information,distance estimation,instance segmentation,monocular vision},
pages = {46059--46070},
publisher = {IEEE},
title = {{Robust Inter-Vehicle Distance Estimation Method Based on Monocular Vision}},
volume = {7},
year = {2019}
}
@article{Pan2019,
abstract = {Global calibration of multi-vision sensors in the railway fields is easily affected by on-site complex environments, such as lighting and self-occlusion, which makes it difficult for existing methods to achieve high-Accuracy calibration. In this paper, a high-Accuracy and flexible calibration method of multi-vision sensors in the outdoor railway fields based on flexible and optimal 3D data field through the combination of articular arm and metal target embedded with luminescent LEDs is proposed. Firstly, a high-Accuracy and flexible 3D data field with multiple angles and views is constructed by the articular arm and the metal target rapidly; Secondly, the high-frequency multi-exposure imaging mode and multi-scale image feature points extraction are adopted, and the high-Accuracy reposition is achieved through Kalman filter, which can reduce the impact of image noise efficiently; Finally, the RANSAC method is utilized to optimize the 3D data field, forming the optimal 3D data field, and the optimal target chains corresponding to each group of cameras are established. The maximum likelihood solution of global parameters is solved by nonlinear optimization. Simulation experiments verify the feasibility of the proposed method. Meanwhile, physical experiment results show this method can reduce the outdoor environment impact and improve the calibration and measurement precision effectively. In addition, the computational efficiency of the proposed method is about 11.2 times than multi-images averaging method, the calibration and measurement accuracy improve about 20 and 2.9 times respectively, which is suitable for the high-Accuracy and flexible global calibration in the complex railway environment.},
author = {Pan, Xiao and Liu, Zhen and Zhang, Guangjun},
doi = {10.1109/ACCESS.2019.2950283},
file = {:home/bruno/Documentos/Mestrado/08886505.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {3D data field,Flexible,global calibration,high-Accuracy,multi-vision sensors,optimal},
pages = {159495--159506},
publisher = {IEEE},
title = {{High-Accuracy Calibration of On-Site Multi-Vision Sensors Based on Flexible and Optimal 3D Field}},
volume = {7},
year = {2019}
}
@article{Wongsaree2018,
abstract = {This paper presents a distance determination technique using an image from the single forward camera. Since too dark or too bright of the image and non-linear relation between height of the object and distance from the camera have effect on the performance of the detection process. Therefore, automatic brightness adjustment and inverse perspective mapping (IMP) is applied in the proposed scheme. In addition, region of interest (ROI) determination is used to decrease the processing time. The experimental results confirm that the proposed technique can detect distance of the object in front of the car where the error is 7.96{\%}.},
author = {Wongsaree, Preaw and Sinchai, Sakkarin and Wardkein, Paramote and Koseeyaporn, Jeerasuda},
doi = {10.1109/CCOMS.2018.8463318},
file = {:home/bruno/Documentos/Mestrado/importante.pdf:pdf},
isbn = {9781538663509},
journal = {2018 3rd International Conference on Computer and Communication Systems, ICCCS 2018},
keywords = {component,distance detection,gamma correction,inverse perspective mapping,opencv},
pages = {323--326},
publisher = {IEEE},
title = {{Distance Detection Technique Using Enhancing Inverse Perspective Mapping}},
year = {2018}
}
@article{Song2013,
abstract = {The object position measuring is an important work of dual-view stereo vision. At present, the most widely used dual-view stereo method of measurement is based on the binocular disparity and geometry between the left and right images. In this paper, we proposed two object position measuring methods based on an adjustable dual-view camera, it requires the optical axes are converged to the same point on the target object. The first method of object position measuring is based on the rotation angles of the two cameras and the pitching angle of the optical axial plane. The second method of object position measuring is based on the relative extrinsic parameters. The proposed methods can perform good position measuring without the epipolar rectification and stereo matching. The validities and reliabilities of the proposed methods are verified by the experimental results. {\textcopyright} 2013 IEEE.},
author = {Song, Xiaowei and Wu, Yuanzhao and Yang, Lei and Liu, Zhong},
doi = {10.1109/ICMEW.2013.6618433},
file = {:home/bruno/Documentos/Mestrado/06618433.pdf:pdf},
isbn = {9781479916047},
journal = {Electronic Proceedings of the 2013 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2013},
keywords = {convergent point,dual-view stereo camera,object position measuring,relative extrinsic parameters},
pages = {1--6},
publisher = {IEEE},
title = {{Object position measuring based on adjustable dual-view camera}},
year = {2013}
}
@article{Bao2016,
abstract = {Self-localization is one of the most important part in autonomous driving system. In urban canyon, the multipath and non-line-of-sight effects to GPS receiver decrease the precision of self-localization of the vehicle. More specifically, the lateral error is more serious because of the blockage of the satellites. However, the building on roadside could be the stable reference object for localization. Therefore, this paper proposes to use stereo camera and 3D building map to reduce the lateral error of positioning result. In our proposal, stereo camera is used to detect and reconstruct the building side view. Lateral distance between building and vehicle estimated by stereo camera is compared with 3D building map to rectify the lateral position of vehicle. In addition, this paper employs inertial sensor and GPS receiver to decide the longitudinal position of vehicle. The particle filter is used for the sensor fusion. The experiment is conducted in the center of Tokyo, Japan, which is a typical urban city scene with high density of tall buildings. It demonstrates that the proposed method could achieve sub-meter level accuracy in GPS difficult environments.},
author = {Bao, Jiali and Gu, Yanlei and Hsu, Li Ta and Kamijo, Shunsuke},
doi = {10.1109/IVS.2016.7535499},
file = {:home/bruno/Documentos/Mestrado/07535499.pdf:pdf},
isbn = {9781509018215},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {June},
pages = {927--932},
title = {{Vehicle self-localization using 3D building map and stereo camera}},
volume = {2016-August},
year = {2016}
}
@article{Tsai2018,
abstract = {This paper describes a simple yet efficient method of obstacle detection. Different from other methods, this study utilizes the coordinate models of and the depth map generated from stereo cameras to accurately detect possible obstacles. The proposed algorithm searches on the depth map along vertical direction for pixels having the same disparity. Having the same disparity indicates that these pixels are from the same object, and such object is an obstacle on the road. After implementation, the average processing time of the proposed obstacle detection algorithm for HD 720p image requires only 4.0 milliseconds (ms) on Intel Core i7 3.6 GHz processor, and 16.3 ms on an embedded system, i.e., the NVIDIA Jetson TX1. The detection performance based on stereo vision is more precise and faster compared with 2-D image object recognition. By directly comparing the purchasing price, the hardware cost to use stereo camera is also much lower than a RADAR or LiDAR system.},
author = {Tsai, Yi Chin and Chen, Kuan Hung and Chen, Yun and Cheng, Jih Hsiang},
doi = {10.1109/VLSI-DAT.2018.8373249},
file = {:home/bruno/Documentos/Mestrado/08373249.pdf:pdf},
isbn = {9781538642603},
journal = {2018 International Symposium on VLSI Design, Automation and Test, VLSI-DAT 2018},
keywords = {automotive,embedded system,obstacle detection,stereo vision},
pages = {1--4},
publisher = {IEEE},
title = {{Accurate and fast obstacle detection method for automotive applications based on stereo vision}},
year = {2018}
}
@article{Weichenberger,
author = {Weichenberger, Lothar and Behn, Tobias},
file = {:home/bruno/Documentos/Mestrado/bruno{\_}hoje.pdf:pdf},
title = {{Camera based vehicle localization in testing scenarios for the evaluation of driving assistance systems}}
}
@article{Huang2018,
author = {Huang, Liqin and Chen, Yanan and Fan, Zhengjia and Chen, Zhifeng},
doi = {10.1117/1.jei.27.4.043019},
file = {:home/bruno/Documentos/Mestrado/043019{\_}1.pdf:pdf},
isbn = {3425842463},
issn = {1560-229X},
journal = {Journal of Electronic Imaging},
keywords = {20,2018,24,28,absolute distance estimation,accepted for publication jun,deep learning,instance segmentation,monocular vision,paper 180230 received mar,published online jul,vehicle classification},
number = {04},
pages = {1},
title = {{Measuring the absolute distance of a front vehicle from an in-car camera based on monocular vision and instance segmentation}},
volume = {27},
year = {2018}
}
@article{Salman2017,
abstract = {Self-driving cars reduce human error and can accomplish various missions to help people in different fields. They have become one of the main interests in automotive research and development, both in the industry and academia. However, many challenges are encountered in dealing with distance measurement and cost, both in equipment and technique. The use of stereo camera to measure the distance of an object is convenient and popular for obstacle avoidance and navigation of autonomous vehicles. The calculation of distance considers angular distance, distance between cameras, and the pixel of the image. This study proposes a method that measures object distance based on trigonometry, that is, facing the self-driving car using image processing and stereo vision with high accuracy, low cost, and computational speed. The method achieves a high distance measuring accuracy of up to 20 m. It can be implemented in real time computing systems and can determine the safe driving distance between obstacles.},
author = {Salman, Yasir Dawood and Ku-mahamud, Ku Ruhana and Kamioka, Eiji},
file = {:home/bruno/Documentos/Mestrado/PID105-235-242e.pdf:pdf},
journal = {Proceeding of the 6Th International Conference of Computing {\&} Informations},
keywords = {distance measurement,image,self-driving car,stereo camera},
number = {105},
pages = {235--242},
title = {{Distance Measurement for Self-Driving Cars Using Stereo Camera}},
year = {2017}
}
@article{Salman2017a,
abstract = {Self-driving cars reduce human error and can accomplish various missions to help people in different fields. They have become one of the main interests in automotive research and development, both in the industry and academia. However, many challenges are encountered in dealing with distance measurement and cost, both in equipment and technique. The use of stereo camera to measure the distance of an object is convenient and popular for obstacle avoidance and navigation of autonomous vehicles. The calculation of distance considers angular distance, distance between cameras, and the pixel of the image. This study proposes a method that measures object distance based on trigonometry, that is, facing the self-driving car using image processing and stereo vision with high accuracy, low cost, and computational speed. The method achieves a high distance measuring accuracy of up to 20 m. It can be implemented in real time computing systems and can determine the safe driving distance between obstacles.},
author = {Salman, Yasir Dawood and Ku-mahamud, Ku Ruhana and Kamioka, Eiji},
file = {:home/bruno/Documentos/Mestrado/PID105-235-242e (1).pdf:pdf},
journal = {Proceeding of the 6Th International Conference of Computing {\&} Informations},
keywords = {distance measurement,image,self-driving car,stereo camera},
number = {105},
pages = {235--242},
title = {{Distance Measurement for Self-Driving Cars Using Stereo Camera}},
year = {2017}
}
@article{Tram2018,
abstract = {This paper proposes an intelligent transport system positioning technique that determines the distance between vehicles via image sensor-based visible light communication. The proposed algorithm uses two image sensors and requires only one LED to estimate the distance. Furthermore, the cameras installed in vehicles are often of an inferior resolution that is insufficient to accurately determine the coordinates of the pixels on the image sensor. This paper addresses this problem by proposing a method that accurately determines the distance between two vehicles when the camera resolution is low. Simulations are conducted to verify the performance of the proposed algorithm.},
author = {Tram, Vo Thi Bich and Yoo, Myungsik},
doi = {10.1109/ACCESS.2018.2793306},
file = {:home/bruno/Documentos/Mestrado/08259282.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Distance estimation,image sensor,resolution,vehicle,visible light communication},
pages = {4521--4527},
publisher = {IEEE},
title = {{Vehicle-To-Vehicle Distance Estimation Using a Low-Resolution Camera Based on Visible Light Communications}},
volume = {6},
year = {2018}
}
@article{Ghobadi2010,
author = {Ghobadi, Seyed Eghbal},
file = {:home/bruno/Documentos/Mestrado/56725747.pdf:pdf},
pages = {141},
title = {{Real Time Object Recognition and Tracking Using 2D/3D Images}},
year = {2010}
}
@article{Rangesh2019,
abstract = {Online multi-object tracking (MOT) is extremely important for high-level spatial reasoning and path planning for autonomous and highly-automated vehicles. In this paper, we present a modular framework for tracking multiple objects (vehicles), capable of accepting object proposals from different sensor modalities (vision and range) and a variable number of sensors, to produce continuous object tracks. This work is a generalization of the MDP framework for MOT, with some key extensions - First, we track objects across multiple cameras and across different sensor modalities. This is done by fusing object proposals across sensors accurately and efficiently. Second, the objects of interest (targets) are tracked directly in the real world. This is a departure from traditional techniques where objects are simply tracked in the image plane. Doing so allows the tracks to be readily used by an autonomous agent for navigation and related tasks. To verify the effectiveness of our approach, we test it on real world highway data collected from a heavily sensorized testbed capable of capturing full-surround information. We demonstrate that our framework is well-suited to track objects through entire maneuvers around the ego-vehicle, some of which take more than a few minutes to complete. We also leverage the modularity of our approach by comparing the effects of including/excluding different sensors, changing the total number of sensors, and the quality of object proposals on the final tracking result.},
archivePrefix = {arXiv},
arxivId = {1802.08755},
author = {Rangesh, Akshay and Trivedi, Mohan Manubhai},
doi = {10.1109/tiv.2019.2938110},
eprint = {1802.08755},
file = {:home/bruno/Documentos/Mestrado/1802.08755.pdf:pdf},
issn = {2379-8858},
journal = {IEEE Transactions on Intelligent Vehicles},
number = {4},
pages = {588--599},
title = {{No Blind Spots: Full-Surround Multi-Object Tracking for Autonomous Vehicles Using Cameras and LiDARs}},
volume = {4},
year = {2019}
}
@article{Unlu2019,
abstract = {Commercial Unmanned aerial vehicle (UAV) industry, which is publicly known as drone, has seen a tremendous increase in last few years, making these devices highly accessible to public. This phenomenon has immediately raised security concerns due to fact that these devices can intentionally or unintentionally cause serious hazards. In order to protect critical locations, the academia and industry have proposed several solutions in recent years. Computer vision is extensively used to detect drones autonomously compared to other proposed solutions such as RADAR, acoustics and RF signal analysis thanks to its robustness. Among these computer vision-based approaches, we see the preference of deep learning algorithms thanks to their effectiveness. In this paper, we are presenting an autonomous drone detection and tracking system which uses a static wide-angle camera and a lower-angle camera mounted on a rotating turret. In order to use memory and time efficiently, we propose a combined multi-frame deep learning detection technique, where the frame coming from the zoomed camera on the turret is overlaid on the wide-angle static camera's frame. With this approach, we are able to build an efficient pipeline where the initial detection of small sized aerial intruders on the main image plane and their detection on the zoomed image plane is performed simultaneously, minimizing the cost of resource exhaustive detection algorithm. In addition to this, we present the integral system including tracking algorithms, deep learning classification architectures and the protocols.},
author = {Unlu, Eren and Zenou, Emmanuel and Riviere, Nicolas and Dupouy, Paul Edouard},
doi = {10.1186/s41074-019-0059-x},
file = {:home/bruno/Documentos/Mestrado/Unlu2019{\_}Article{\_}DeepLearning-basedStrategiesFo.pdf:pdf},
issn = {18826695},
journal = {IPSJ Transactions on Computer Vision and Applications},
keywords = {Deep learning,Surveillance,Unmanned aerial vehicle},
number = {1},
publisher = {IPSJ Transactions on Computer Vision and Applications},
title = {{Deep learning-based strategies for the detection and tracking of drones using several cameras}},
volume = {11},
year = {2019}
}
@article{Sankaranarayanan2008,
abstract = {Video cameras are among the most commonly used sensors in a large number of applications, ranging from surveillance to smart rooms for videoconferencing. There is a need to develop algorithms for tasks such as detection, tracking, and recognition of objects, specifically using distributed networks of cameras. The projective nature of imaging sensors provides ample challenges for data association across cameras. We first discuss the nature of these challenges in the context of visual sensor networks. Then, we show how real-world constraints can be favorably exploited in order to tackle these challenges. Examples of real-world constraints are a) the presence of a world plane, b) the presence of a three-dimiensional scene model, c) consistency of motion across cameras, and d) color and texture properties. In this regard, the main focus of this paper is towards highlightingthe efficient use of the geometric constraints induced by the imaging devices to derive distributed algorithms for target detection, tracking, and recognition. Our discussions are supported by several examples drawn from real applications. Lastly, we also describe several potential research problems that remain to be addressed. {\textcopyright} 2008 IEEE.},
author = {Sankaranarayanan, Aswin C. and Veeraraghavan, Ashok and Chellappa, Rama},
doi = {10.1109/JPROC.2008.928758},
file = {:home/bruno/Documentos/Mestrado/Sank{\_}PIEEE{\_}2008.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Detection,Distributed sensing,Geometric constraints,Multiview geometry,Recognition,Smart cameras,Tracking},
number = {10},
pages = {1606--1624},
title = {{Object detection, tracking and recognition for multiple smart cameras}},
volume = {96},
year = {2008}
}
@article{Skulimowski2012,
abstract = {This paper presents the theoretical foundations of an intelligent on-line modelling tool capable of processing heterogeneous information on complex techno-economical systems. Its main functionality is to investigate, elicit, and apply rules and principles that govern the development processes of technologies and related markets. Specifically, we will focus on applications of the tool to model the evolution of information technology (IT). We will distinguish several relevant subsystems of the system under study, which describe the demographic, education, global economic trends, as well as specific market factors that determine the demand for and use of IT. The group modelling techniques are implemented in the new tool to enable the collaborative and distributed model building with intelligent verification of entries called 'model wiki'. Based on the information elicited from experts, gathered from the web and professional databases, a discrete-time control model of technological evolution emerges, coupled with a controlled discrete-event system. The latter processes qualitative information and models the influence of external events and trends on the discrete-time control system parameters. We propose novel uncertainty handling techniques capable of processing and combining different types of uncertain information, coming i.a. from Delphi research and forecasts. The quantitative information is dynamically updated by autonomous webcrawlers, following an adaptive intelligent strategy. The resulting model can be used to simulate long-term future trends and scenarios. Its ultimate goal is to perform an optimization process and derive recommendations for decision makers, for example when selecting IT investment strategies in an innovative enterprise. {\textcopyright} 2012 Springer-Verlag.},
author = {Skulimowski, Andrzej M J},
doi = {10.1007/978-3-642-31919-8},
file = {:home/bruno/Documentos/Mestrado/Camera{\_}Array{\_}Synthetic{\_}Aperture{\_}Focusing{\_}and{\_}Fusio.pdf:pdf},
isbn = {978-3-642-31918-1},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Complex Systems,Decision Support Systems,Discrete-Time Control,Foresight,Group Modelling Tool,Hybrid Models,Model Discovery},
number = {February 2015},
pages = {614--626},
title = {{Intelligent Science and Intelligent Data Engineering}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84865808451{\&}partnerID=tZOtx3y1},
volume = {7202},
year = {2012}
}
@article{Real2019,
abstract = {We propose V2CNet, a new deep learning framework to automatically translate the demonstration videos to commands that can be directly used in robotic applications. Our V2CNet has two branches and aims at understanding the demonstration video in a fine-grained manner. The first branch has the encoder-decoder architecture to encode the visual features and sequentially generate the output words as a command, while the second branch uses a Temporal Convolutional Network (TCN) to learn the fine-grained actions. By jointly training both branches, the network is able to model the sequential information of the command, while effectively encodes the fine-grained actions. The experimental results on our new large-scale dataset show that V2CNet outperforms recent state-of-the-art methods by a substantial margin, while its output can be applied in real robotic applications. The source code and trained models will be made available.},
archivePrefix = {arXiv},
arxivId = {arXiv:1910.13317v1},
author = {Real, Fabio and Batou, Anas and Ritto, Thiago and Desceliers, Christophe},
doi = {10.1177/ToBeAssigned},
eprint = {arXiv:1910.13317v1},
file = {:home/bruno/Documentos/Mestrado/1910.13317.pdf:pdf},
journal = {Journal of Vibration and Control},
keywords = {[PHYS.MECA.VIBR]Physics [physics]/Mechanics [physics]/Vibrations [physics.class-ph],bit-rock stochastic interaction model,drillstring dynamics,experimental identification,hysteretic friction},
pages = {107754631982824},
title = {{Stochastic modeling for hysteretic bit–rock interaction of a drill string under torsional vibrations}},
year = {2019}
}
@article{Wu2019,
abstract = {Computer-vision methods have been extensively used in intelligent transportation systems for vehicle detection. However, the detection of severely occluded or partially observed vehicles due to the limited camera fields of view remains a challenge. This paper presents a multi-camera vehicle detection system that significantly improves the detection performance under occlusion conditions. The key elements of the proposed method include a novel multi-view region proposal network that localizes the candidate vehicles on the ground plane. We also infer the vehicle position on the ground plane by leveraging multi-view cross-camera context. Experiments are conducted on dataset captured from a roadway in Richardson, TX, USA, and the system attains 0.7849 Average Precision and 0.7089 Multi Object Detection Precision. The proposed system results in an approximately 31.2{\%} increase in AP and 8.6{\%} in MODP than the single-camera methods.},
author = {Wu, Hao and Zhang, Xinxiang and Story, Brett and Rajan, Dinesh},
doi = {10.1109/ICASSP.2019.8683350},
file = {:home/bruno/Documentos/Mestrado/08683350.pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Vehicle detection,multi-camera,multi-view fusion.,region proposal network,vehicle occlusion},
pages = {3767--3771},
title = {{Accurate Vehicle Detection Using Multi-camera Data Fusion and Machine Learning}},
volume = {2019-May},
year = {2019}
}
@article{Pei2019,
abstract = {With the three-dimensional (3D) coordinates of objects captured by a sequence of images taken in different views, object reconstruction is a technique which aims to recover the shape and appearance information of objects. Although great progress in object reconstruction has been made over the past few years, object reconstruction in occlusion situations remains a challenging problem. In this paper, we propose a novel method to reconstruct occluded objects based on synthetic aperture imaging. Unlike most existing methods, which either assume that there is no occlusion in the scene or remove the occlusion from the reconstructed result, our method uses the characteristics of synthetic aperture imaging that can effectively reduce the influence of occlusion to reconstruct the scene with occlusion. The proposed method labels occlusion pixels according to variance and reconstructs the 3D point cloud based on synthetic aperture imaging. Accuracies of the point cloud are tested by calculating the spatial difference between occlusion and non-occlusion conditions. The experiment results show that the proposed method can handle the occluded situation well and demonstrates a promising performance.},
author = {Pei, Zhao and Li, Yawen and Ma, Miao and Li, Jun and Leng, Chengcai and Zhang, Xiaoqiang and Zhang, Yanning},
doi = {10.3390/s19030607},
file = {:home/bruno/Documentos/Mestrado/sensors-19-00607.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Occluded-object 3D reconstruction,Synthetic aperture imaging},
number = {3},
pages = {1--22},
title = {{Occluded-object 3D reconstruction using camera array synthetic aperture imaging}},
volume = {19},
year = {2019}
}
@article{Zaarane2020,
abstract = {The focus of this paper is inter-vehicles distance measurement which is a very important and challenging task in image processing domain. Where it is used in several systems such as Driving Safety Support Systems (DSSS), autonomous driving and traffic mobility. In the current paper, we propose an inter-vehicle distance measurement system for self-driving based on image processing. The proposed system uses two cameras mounted as one stereo camera, in the hosting vehicle behind the rear-view mirror. The detection of vehicles is performed first in a single camera using a recent powerful work from the literature. Then, the same vehicle is detected in the image captured by the second camera using template matching technique. Thus, the inter-vehicle distance is calculated using a simple method based on the position of the vehicle in both cameras, geometric derivations and additional technical data such as distance between the cameras and some other specific angles (e.g. the cameras view field angle). The results of the extensive experiments showed the high accuracy of the proposed method compared to the previous works from literature and it allows to measure efficiently the distances between the vehicles and the hosting vehicle. In addition, this method could be used in several systems of various domains in real time regardless of the object types. The experiments results were done on a Hardware Processor System (HPS) located in a VEEK-MT2S provided by TERASIC.},
author = {Zaarane, Abdelmoghit and Slimani, Ibtissam and {Al Okaishi}, Wahban and Atouf, Issam and Hamdoun, Abdellatif},
doi = {10.1016/j.array.2020.100016},
file = {:home/bruno/Documentos/Mestrado/1-s2.0-S2590005620300011-main.pdf:pdf},
issn = {25900056},
journal = {Array},
keywords = {distance measurement},
number = {May 2019},
pages = {100016},
publisher = {Elsevier Ltd},
title = {{Distance measurement system for autonomous vehicles using stereo camera}},
url = {https://doi.org/10.1016/j.array.2020.100016},
volume = {5},
year = {2020}
}
@article{Vankrunkelsven,
author = {Vankrunkelsven, Vincent},
file = {:home/bruno/Documentos/Mestrado/chapter3.pdf:pdf},
title = {{Vincent Vankrunkelsven}}
}
@article{Mallot1991,
abstract = {We present a scheme for obstacle detection from optical flow which is based on strategies of biological information processing. Optical flow is established by a local "voting" (non-maximum suppression) over the outputs of correlation-type motion detectors similar to those found in the fly visual system. The computational theory of obstacle detection is discussed in terms of space-variances of the motion field. An efficient mechanism for the detection of disturbances in the expected motion field is based on "inverse perspective mapping", i.e., a coordinate transform or retinotopic mapping applied to the image. It turns out that besides obstacle detection, inverse perspective mapping has additional advantages for regularizing optical flow algorithms. Psychophysical evidence for body-scaled obstacle detection and related neurophysiological results are discussed. {\textcopyright} 1991 Springer-Verlag.},
author = {Mallot, Hanspeter A. and B{\"{u}}lthoff, H. H. and Little, J. J. and Bohrer, S.},
doi = {10.1007/BF00201978},
file = {:home/bruno/Documentos/Mestrado/MallotBulLitBoh{\_}BiolCyb91.pdf:pdf},
issn = {03401200},
journal = {Biological Cybernetics},
number = {3},
pages = {177--185},
title = {{Inverse perspective mapping simplifies optical flow computation and obstacle detection}},
volume = {64},
year = {1991}
}
@article{Tuohy2010,
abstract = {This paper presents a novel real-time distance determination algorithm using an image sensor for use in an automobile environment. The system uses a forward facing camera placed within the vehicle. From a single forward facing image, it is difficult to determine distances to objects in front of the vehicle with any degree of certainty. There is a non linear relationship between the height of an object in a forward facing image and its distance from the camera. This paper presents a method which uses Inverse Perspective Mapping (IPM) to overcome this problem. Using IPM, we can transform the forward facing image to a top-down "bird's eye" view, in which there is a linear relationship between distances in the image and in the real world. The algorithm is implemented in the C language using the OpenCV libraries. Implementation in OpenCV leads to a high performance, low overhead system that could be implemented on a low power embedded device in an automotive environment.},
author = {Tuohy, S. and O'Cualain, D. and Jones, E. and Glavin, M.},
doi = {10.1049/cp.2010.0495},
file = {:home/bruno/Documentos/Mestrado/docualain{\_}issc10 (1).pdf:pdf},
isbn = {9781849192521},
journal = {IET Conference Publications},
keywords = {Distance Detection,Inverse Perspective Mapping,OpenCV},
number = {566 CP},
pages = {100--105},
title = {{Distance determination for an automobile environment using inverse perspective mapping in OpenCV}},
volume = {2010},
year = {2010}
}
@article{Sivaraman2013,
abstract = {In this paper, we introduce a synergistic approach to integrated lane and vehicle tracking for driver assistance. The approach presented in this paper results in a final system that improves on the performance of both lane tracking and vehicle tracking modules. Further, the presented approach introduces a novel approach to localizing and tracking other vehicles on the road with respect to lane position, which provides information on higher contextual relevance that neither the lane tracker nor vehicle tracker can provide by itself. Improvements in lane tracking and vehicle tracking have been extensively quantified. Integrated system performance has been validated on real-world highway data. Without specific hardware and software optimizations, the fully implemented system runs at near-real-time speeds of 11 frames per second. {\textcopyright} 2000-2011 IEEE.},
author = {Sivaraman, Sayanan and Trivedi, Mohan Manubhai},
doi = {10.1109/TITS.2013.2246835},
file = {:home/bruno/Documentos/Mestrado/SayananTrivedi{\_}IEEETITS-june2013.pdf:pdf},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Active safety,computer vision,driver assistance,intelligent vehicles,lane departure,lane tracking,vehicle tracking},
number = {2},
pages = {906--917},
title = {{Integrated lane and vehicle detection, localization, and tracking: A synergistic approach}},
volume = {14},
year = {2013}
}
@article{Lin2014,
abstract = {This paper presents a novel method of camera parameters calibration for obstacle distance measurement based on monocular vision. In this method, resolving camera parameters have been decomposed into two linear equation sets, which has significantly reduced computing complexity and improved computing speed. The ideal pinhole imaging model of obstacle has been demonstrated, then the linear equations have been derived afterwards. In the end, experiment shows that the proposed method is very effective. {\textcopyright} 2014 IEEE.},
author = {Lin, Chenchen and Su, Fulin and Wang, Haitao and Gao, Jianjun},
doi = {10.1109/CSNT.2014.233},
file = {:home/bruno/Documentos/Mestrado/06821579.pdf:pdf},
isbn = {9781479930708},
journal = {Proceedings - 2014 4th International Conference on Communication Systems and Network Technologies, CSNT 2014},
keywords = {Camera calibration,Computer vision,Distance measurement,Monocular vision},
pages = {1148--1151},
publisher = {IEEE},
title = {{A camera calibration method for obstacle distance measurement based on monocular vision}},
year = {2014}
}
@article{Ramesh2018,
abstract = {The aim of this research is to design an intelligent system that addresses the problem of real-time localization and navigation of visually impaired (VI) in an indoor environment using a monocular camera. Systems that have been developed so far for the VI use either many cameras (stereo and monocular) integrated with other sensors or use very complex algorithms that are computationally expensive. In this research work, a computationally less expensive integrated system has been proposed to combine imaging geometry, Visual Odometry (VO), Object Detection (OD) along with Distance-Depth (D-D) estimation algorithms for precise navigation and localization by utilizing a single monocular camera as the only sensor. The developed algorithm is tested for both standard Karlsruhe and indoor environment recorded datasets. Tests have been carried out in real-time using a smartphone camera that captures image data of the environment as the person moves and is sent over Wi-Fi for further processing to the MATLAB software model running on an Intel i7 processor. The algorithm provides accurate results on real-time navigation in the environment with an audio feedback about the person's location. The trajectory of the navigation is expressed in an arbitrary scale. Object detection based localization is accurate. The D-D estimation provides distance and depth measurements up to an accuracy of 94-98{\%}.},
author = {Ramesh, Kruthika and Nagananda, S. N. and Ramasangu, Hariharan and Deshpande, Rohini},
doi = {10.1109/IEA.2018.8387082},
file = {:home/bruno/Documentos/Mestrado/08387082.pdf:pdf},
isbn = {9781538657478},
journal = {2018 5th International Conference on Industrial Engineering and Applications, ICIEA 2018},
keywords = {KLT tracker,image processing,monocular camera,optical flow,pattern recognition,visual SLAM,visual odometry},
pages = {122--128},
publisher = {IEEE},
title = {{Real-time localization and navigation in an indoor environment using monocular camera for visually impaired}},
year = {2018}
}
@article{Simon2019,
abstract = {Lidar based 3D object detection is inevitable for autonomous driving, because it directly links to environmental understanding and therefore builds the base for prediction and motion planning. The capacity of inferencing highly sparse 3D data in real-time is an ill-posed problem for lots of other application areas besides automated vehicles, e.g. augmented reality, personal robotics or industrial automation. We introduce Complex-YOLO, a state of the art real-time 3D object detection network on point clouds only. In this work, we describe a network that expands YOLOv2, a fast 2D standard object detector for RGB images, by a specific complex regression strategy to estimate multi-class 3D boxes in Cartesian space. Thus, we propose a specific Euler-Region-Proposal Network (E-RPN) to estimate the pose of the object by adding an imaginary and a real fraction to the regression network. This ends up in a closed complex space and avoids singularities, which occur by single angle estimations. The E-RPN supports to generalize well during training. Our experiments on the KITTI benchmark suite show that we outperform current leading methods for 3D object detection specifically in terms of efficiency. We achieve state of the art results for cars, pedestrians and cyclists by being more than five times faster than the fastest competitor. Further, our model is capable of estimating all eight KITTI-classes, including Vans, Trucks or sitting pedestrians simultaneously with high accuracy.},
author = {Simon, Martin and Milz, Stefan and Amende, Karl and Gross, Horst Michael},
doi = {10.1007/978-3-030-11009-3_11},
file = {:home/bruno/Documentos/Mestrado/Simony{\_}Complex-YOLO{\_}An{\_}Euler-Region-Proposal{\_}for{\_}Real-time{\_}3D{\_}Object{\_}Detection{\_}on{\_}Point{\_}ECCVW{\_}2018{\_}paper.pdf:pdf},
isbn = {9783030110086},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {3D object detection,Autonomous driving,Lidar,Point cloud processing},
pages = {197--209},
title = {{Complex-YOLO: An euler-region-proposal for real-time 3D object detection on point clouds}},
volume = {11129 LNCS},
year = {2019}
}
@article{Kim2019,
abstract = {In this paper, we propose weighted-mean YOLO to improve real-time performance of object detection by fusing information of RGB camera and LIDAR. RGB camera is vulnerable to external environments and therefore strongly affected by illumination. Conversely, LIDAR is robust to external environments, but has low resolution. Since each sensor can complement their disadvantages, we propose a method to improve the performance of object detection through sensor fusion. We design the system using weighted-mean to construct a robust system and compared with other algorithms, it shows performance improvement of missed-detection.},
author = {Kim, Jinsoo and Kim, Jongwon and Cho, Jeongho},
doi = {10.1109/ICSPCS47537.2019.9008742},
file = {:home/bruno/Documentos/Mestrado/09008742.pdf:pdf},
isbn = {9781728121949},
journal = {2019, 13th International Conference on Signal Processing and Communication Systems, ICSPCS 2019 - Proceedings},
keywords = {LIDAR,YOLO,object detection,real-time,sensor fusion},
pages = {1--5},
publisher = {IEEE},
title = {{An advanced object classification strategy using YOLO through camera and LiDAR sensor fusion}},
year = {2019}
}
@article{Simon2019a,
abstract = {Accurate detection of 3D objects is a fundamental problem in computer vision and has an enormous impact on autonomous cars, augmented/virtual reality and many applications in robotics. In this work we present a novel fusion of neural network based state-of-the-art 3D detector and visual semantic segmentation in the context of autonomous driving. Additionally, we introduce Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable evaluation metric for comparison of object detections, which speeds up our inference time up to 20$\backslash${\%} and halves training time. On top, we apply state-of-the-art online multi target feature tracking on the object measurements to further increase accuracy and robustness utilizing temporal information. Our experiments on KITTI show that we achieve same results as state-of-the-art in all related categories, while maintaining the performance and accuracy trade-off and still run in real-time. Furthermore, our model is the first one that fuses visual semantic with 3D object detection.},
archivePrefix = {arXiv},
arxivId = {1904.07537},
author = {Simon, Martin and Amende, Karl and Kraus, Andrea and Honer, Jens and S{\"{a}}mann, Timo and Kaulbersch, Hauke and Milz, Stefan and Gross, Horst Michael},
eprint = {1904.07537},
file = {:home/bruno/Documentos/Mestrado/Simon{\_}Complexer-YOLO{\_}Real-Time{\_}3D{\_}Object{\_}Detection{\_}and{\_}Tracking{\_}on{\_}Semantic{\_}Point{\_}CVPRW{\_}2019{\_}paper.pdf:pdf},
title = {{Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds}},
url = {http://arxiv.org/abs/1904.07537},
year = {2019}
}
@article{Oliveira2015,
abstract = {Over the past years, inverse perspective mapping has been successfully applied to several problems in the field of Intelligent Transportation Systems. In brief, the method consists of mapping images to a new coordinate system where perspective effects are removed. The removal of perspective associated effects facilitates road and obstacle detection and also assists in free space estimation. There is, however, a significant limitation in the inverse perspective mapping: the presence of obstacles on the road disrupts the effectiveness of the mapping. The current paper proposes a robust solution based on the use of multimodal sensor fusion. Data from a laser range finder is fused with images from the cameras, so that the mapping is not computed in the regions where obstacles are present. As shown in the results, this considerably improves the effectiveness of the algorithm and reduces computation time when compared with the classical inverse perspective mapping. Furthermore, the proposed approach is also able to cope with several cameras with different lenses or image resolutions, as well as dynamic viewpoints.},
author = {Oliveira, Miguel and Santos, Vitor and Sappa, Angel D.},
doi = {10.1016/j.inffus.2014.09.003},
file = {:home/bruno/Documentos/Mestrado/J{\_}{\_}Elsevier{\_}IF{\_}Vol{\_}24{\_}July{\_}2015{\_}pp{\_}108-121.pdf:pdf},
issn = {15662535},
journal = {Information Fusion},
keywords = {Intelligent vehicles,Inverse perspective mapping,Multimodal sensor fusion},
pages = {108--121},
publisher = {Elsevier B.V.},
title = {{Multimodal inverse perspective mapping}},
url = {http://dx.doi.org/10.1016/j.inffus.2014.09.003},
volume = {24},
year = {2015}
}
@misc{,
file = {:home/bruno/Documentos/Mestrado/bandbA1{\_}2.pdf:pdf},
title = {{bandbA1{\_}2.pdf}}
}
@article{No2001,
abstract = {Inhibitor for delta5 desaturase},
author = {No, Patent},
file = {:home/bruno/Documentos/Mestrado/US10192312.pdf:pdf},
journal = {Young},
number = {12},
pages = {0--2},
title = {{States Patent}},
volume = {1},
year = {2001}
}
@misc{,
file = {:home/bruno/Documentos/Mestrado/bandbA1{\_}2.pdf:pdf},
title = {{bandbA1{\_}2.pdf}}
}
