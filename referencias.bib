@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@article{lecun1995convolutional,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995}
}


@article{Hane2017,
abstract = {Cameras are a crucial exteroceptive sensor for self-driving cars as they are low-cost and small, provide appearance information about the environment, and work in various weather conditions. They can be used for multiple purposes such as visual navigation and obstacle detection. We can use a surround multi-camera system to cover the full 360-degree field-of-view around the car. In this way, we avoid blind spots which can otherwise lead to accidents. To minimize the number of cameras needed for surround perception, we utilize fisheye cameras. Consequently, standard vision pipelines for 3D mapping, visual localization, obstacle detection, etc. need to be adapted to take full advantage of the availability of multiple cameras rather than treat each camera individually. In addition, processing of fisheye images has to be supported. In this paper, we describe the camera calibration and subsequent processing pipeline for multi-fisheye-camera systems developed as part of the V-Charge project. This project seeks to enable automated valet parking for self-driving cars. Our pipeline is able to precisely calibrate multi-camera systems, build sparse 3D maps for visual navigation, visually localize the car with respect to these maps, generate accurate dense maps, as well as detect obstacles based on real-time depth map extraction.},
archivePrefix = {arXiv},
arxivId = {1708.09839},
author = {H{\"{a}}ne, Christian and Heng, Lionel and Lee, Gim Hee and Fraundorfer, Friedrich and Furgale, Paul and Sattler, Torsten and Pollefeys, Marc},
doi = {10.1016/j.imavis.2017.07.003},
eprint = {1708.09839},
file = {:home/bruno/Documentos/Mestrado/3D{\_}Visual{\_}Perception{\_}for{\_}Self-Driving{\_}Cars{\_}using{\_}a.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Calibration,Fisheye camera,Localization,Mapping,Multi-camera system,Obstacle detection},
number = {August},
pages = {14--27},
title = {{3D visual perception for self-driving cars using a multi-camera system: Calibration, mapping, localization, and obstacle detection}},
volume = {68},
year = {2017}
}

@inproceedings{Daniel,
  title={Inference of driver behavior using correlated IoT data from the vehicle telemetry and the driver mobile phone},
  author={da Silva, Daniel Alves and Torres, Jos{\'e} Alberto Sousa and Pinheiro, Alexandre and de Caldas Filho, Francisco L and Mendon{\c{c}}a, Fabio LL and Praciano, Bruno JG and de Oliveira Kfouri, Guilherme and de Sousa, Rafael T},
  booktitle={2019 Federated Conference on Computer Science and Information Systems (FedCSIS)},
  pages={487--491},
  year={2019},
  organization={IEEE}
}


@article{Lin2014,
abstract = {This paper presents a novel method of camera parameters calibration for obstacle distance measurement based on monocular vision. In this method, resolving camera parameters have been decomposed into two linear equation sets, which has significantly reduced computing complexity and improved computing speed. The ideal pinhole imaging model of obstacle has been demonstrated, then the linear equations have been derived afterwards. In the end, experiment shows that the proposed method is very effective. {\textcopyright} 2014 IEEE.},
author = {Lin, Chenchen and Su, Fulin and Wang, Haitao and Gao, Jianjun},
doi = {10.1109/CSNT.2014.233},
file = {:home/bruno/Documentos/Mestrado/06821579.pdf:pdf},
isbn = {9781479930708},
journal = {Proceedings - 2014 4th International Conference on Communication Systems and Network Technologies, CSNT 2014},
keywords = {Camera calibration,Computer vision,Distance measurement,Monocular vision},
pages = {1148--1151},
publisher = {IEEE},
title = {{A camera calibration method for obstacle distance measurement based on monocular vision}},
year = {2014}
}
@article{Li2020,
abstract = {As an important infrastructure, road has attracted a lot of attentions in recent years. Most of current road information extraction methods rely on either elaborate algorithms or complex equipment. This manuscript proposes a simple and rapid image mosaic method to extract road surface based on a monocular camera. First, key frames are extracted to remove redundant information from image sequences. To eliminate perspective effect, extracted key frames are transformed into top view of road images based on inverse perspective mapping algorithm with the help of an attitude tensor. Then, a coarse-to-fine registration strategy is proposed to align all transformed key frames into a unified coordinate system. Finally, considering the specificity of transformed images, a superposing and overlapping image fusion strategy is proposed to alleviate the effect of stitching seam. The experiments are conducted on a road with the size of 128×17 meters, and the experimental results demonstrate the effectiveness and efficiency of proposed method. In conclusion, the proposed method is simple, effective and efficient and can be applied in a wide range of applications such as large scale road surface diagnose.},
author = {Li, Wunan and Cao, Yu},
doi = {10.1088/1742-6596/1437/1/012008},
file = {:home/bruno/Downloads/Li{\_}2020{\_}J.{\_}Phys.{\_}{\_}Conf.{\_}Ser.{\_}1437{\_}012008.pdf:pdf},
issn = {17426596},
journal = {Journal of Physics: Conference Series},
number = {1},
title = {{A Rapid Road Image Mosaic Method Based on Monocular Camera}},
volume = {1437},
year = {2020}
}
@article{Tsai2018,
abstract = {This paper describes a simple yet efficient method of obstacle detection. Different from other methods, this study utilizes the coordinate models of and the depth map generated from stereo cameras to accurately detect possible obstacles. The proposed algorithm searches on the depth map along vertical direction for pixels having the same disparity. Having the same disparity indicates that these pixels are from the same object, and such object is an obstacle on the road. After implementation, the average processing time of the proposed obstacle detection algorithm for HD 720p image requires only 4.0 milliseconds (ms) on Intel Core i7 3.6 GHz processor, and 16.3 ms on an embedded system, i.e., the NVIDIA Jetson TX1. The detection performance based on stereo vision is more precise and faster compared with 2-D image object recognition. By directly comparing the purchasing price, the hardware cost to use stereo camera is also much lower than a RADAR or LiDAR system.},
author = {Tsai, Yi Chin and Chen, Kuan Hung and Chen, Yun and Cheng, Jih Hsiang},
doi = {10.1109/VLSI-DAT.2018.8373249},
file = {:home/bruno/Documentos/Mestrado/08373249.pdf:pdf},
isbn = {9781538642603},
journal = {2018 International Symposium on VLSI Design, Automation and Test, VLSI-DAT 2018},
keywords = {automotive,embedded system,obstacle detection,stereo vision},
pages = {1--4},
publisher = {IEEE},
title = {{Accurate and fast obstacle detection method for automotive applications based on stereo vision}},
year = {2018}
}
@article{Wu2019,
abstract = {Computer-vision methods have been extensively used in intelligent transportation systems for vehicle detection. However, the detection of severely occluded or partially observed vehicles due to the limited camera fields of view remains a challenge. This paper presents a multi-camera vehicle detection system that significantly improves the detection performance under occlusion conditions. The key elements of the proposed method include a novel multi-view region proposal network that localizes the candidate vehicles on the ground plane. We also infer the vehicle position on the ground plane by leveraging multi-view cross-camera context. Experiments are conducted on dataset captured from a roadway in Richardson, TX, USA, and the system attains 0.7849 Average Precision and 0.7089 Multi Object Detection Precision. The proposed system results in an approximately 31.2{\%} increase in AP and 8.6{\%} in MODP than the single-camera methods.},
author = {Wu, Hao and Zhang, Xinxiang and Story, Brett and Rajan, Dinesh},
doi = {10.1109/ICASSP.2019.8683350},
file = {:home/bruno/Documentos/Mestrado/08683350.pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Vehicle detection,multi-camera,multi-view fusion.,region proposal network,vehicle occlusion},
pages = {3767--3771},
title = {{Accurate Vehicle Detection Using Multi-camera Data Fusion and Machine Learning}},
volume = {2019-May},
year = {2019}
}
@article{Kim2019,
abstract = {In this paper, we propose weighted-mean YOLO to improve real-time performance of object detection by fusing information of RGB camera and LIDAR. RGB camera is vulnerable to external environments and therefore strongly affected by illumination. Conversely, LIDAR is robust to external environments, but has low resolution. Since each sensor can complement their disadvantages, we propose a method to improve the performance of object detection through sensor fusion. We design the system using weighted-mean to construct a robust system and compared with other algorithms, it shows performance improvement of missed-detection.},
author = {Kim, Jinsoo and Kim, Jongwon and Cho, Jeongho},
doi = {10.1109/ICSPCS47537.2019.9008742},
file = {:home/bruno/Documentos/Mestrado/09008742.pdf:pdf},
isbn = {9781728121949},
journal = {2019, 13th International Conference on Signal Processing and Communication Systems, ICSPCS 2019 - Proceedings},
keywords = {LIDAR,YOLO,object detection,real-time,sensor fusion},
pages = {1--5},
publisher = {IEEE},
title = {{An advanced object classification strategy using YOLO through camera and LiDAR sensor fusion}},
year = {2019}
}
@article{Verma2015,
abstract = {In real world application, video security is becoming more important now-a-days due to the happening of unwanted events in our surroundings. Moving object detection is a challenging task in low resolution video, variable lightening conditions and in crowed area due to the limitation of pattern recognition techniques and itlooses many important details in the visual appearance of the moving object. In this paper we propose a review on unusual event detection in video surveillance system. Video surveillance system might be used for enhancing the security in various organizations, academic institutions and many more areas.},
author = {Verma, Kamal Kant and Kumar, Pradeep and Tomar, Ankit},
file = {:home/bruno/Downloads/10.1.1.645.7492.pdf:pdf},
isbn = {9789380544168},
journal = {2015 International Conference on Computing for Sustainable Global Development, INDIACom 2015},
keywords = {Low resolution video,Moving object detection,Unusual event detection,Variable lightening conditions,Video surveillance},
number = {3},
pages = {1758--1762},
title = {{Analysis of moving object detection and tracking in video surveillance system}},
year = {2015}
}
@article{Hewitt2019,
abstract = {We introduce the Autonomous Vehicle Acceptance Model (AVAM), a model of user acceptance for autonomous vehicles, adapted from existing models of user acceptance for generic technologies. A 26-item questionnaire is developed in accordance with the model and a survey conducted to evaluate 6 autonomy scenarios. In a pilot survey (n = 54) and follow-up survey (n = 187), the AVAM presented good internal consistency and replicated patterns from previous surveys. Results showed that users were less accepting of high autonomy levels and displayed significantly lower intention to use highly autonomous vehicles. We also assess expected driving engagement of hands, feet and eyes which are shown to be lower for full autonomy compared with all other autonomy levels. This highlighted that partial autonomy, regardless of level, is perceived to require uniformly higher driver engagement than full autonomy. These results can inform experts regarding public perception of autonomy across SAE levels. The AVAM and associated questionnaire enable standardised evaluation of AVs across studies, allowing for meaningful assessment of changes in perception over time and between different technologies.},
author = {Hewitt, Charlie and Amanatidis, Theocharis and Politis, Ioannis and Sarkar, Advait},
doi = {10.1145/3301275.3302268},
file = {:home/bruno/Downloads/3301275.3302268.pdf:pdf},
isbn = {9781450362726},
journal = {International Conference on Intelligent User Interfaces, Proceedings IUI},
keywords = {Acceptance Model,Autonomous Cars,Methods of Engagement,Questionnaire,User Acceptance},
pages = {518--527},
title = {{Assessing public perception of self-driving cars: The autonomous vehicle acceptance model}},
volume = {Part F147615},
year = {2019}
}
@article{Febbo2018,
author = {Febbo, Huckleberry},
file = {:home/bruno/Downloads/08951131.pdf:pdf},
journal = {Defense},
number = {9},
pages = {986--992},
publisher = {IEEE},
title = {{Autonomous Vehicle Control Documentation}},
year = {2018}
}
@article{Morsu,
author = {Morsu},
file = {:home/bruno/Documentos/Mestrado/morsu.pdf:pdf},
pages = {1--10},
title = {{Camera Based Moving Object Detection for Autonomous Driving}}
}
@article{Weichenberger,
author = {Weichenberger, Lothar and Behn, Tobias},
file = {:home/bruno/Documentos/Mestrado/bruno{\_}hoje.pdf:pdf},
title = {{Camera based vehicle localization in testing scenarios for the evaluation of driving assistance systems}}
}
@article{Guillet2020,
abstract = {Large collections of images have become readily available through modern digital catalogs, from sources as diverse as historical photographs, aerial surveys, or user-contributed pictures. Exploiting the quantitative information present in such wide-ranging collections can greatly benefit studies that follow the evolution of landscape features over decades, such as measuring areas of glaciers to study their shrinking under climate change. However, many available images were taken with low-quality lenses and unknown camera parameters. Useful quantitative data may still be extracted, but it becomes important to both account for imperfect optics, and estimate the uncertainty of the derived quantities. In this paper, we present a method to address both these goals, and apply it to the estimation of the area of a landscape feature traced as a polygon on the image of interest. The technique is based on a Bayesian formulation of the camera calibration problem. First, the probability density function (PDF) of the unknown camera parameters is determined for the image, based on matches between 2D (image) and 3D (world) points together with any available prior information. In a second step, the posterior distribution of the feature area of interest is derived from the PDF of camera parameters. In this step, we also model systematic errors arising in the polygon tracing process, as well as uncertainties in the digital elevation model. The resulting area PDF therefore accounts for most sources of uncertainty. We present validation experiments, and show that the model produces accurate and consistent results. We also demonstrate that in some cases, accounting for optical lens distortions is crucial for accurate area determination with consumer-grade lenses. The technique can be applied to many other types of quantitative features to be extracted from photographs when careful error estimation is important.},
author = {Guillet, Gr{\'{e}}goire and Guillet, Thomas and Ravanel, Ludovic},
doi = {10.1016/j.isprsjprs.2019.11.013},
file = {:home/bruno/Downloads/1-s2.0-S0924271619302734-main.pdf:pdf},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Bayesian methods,Camera calibration,Digital elevation models,Inverse perspective,Lens distortion,Spatial resection},
number = {February 2019},
pages = {237--255},
publisher = {Elsevier},
title = {{Camera orientation, calibration and inverse perspective with uncertainties: A Bayesian method applied to area estimation from diverse photographs}},
url = {https://doi.org/10.1016/j.isprsjprs.2019.11.013},
volume = {159},
year = {2020}
}
@article{Tang2019,
abstract = {Urban traffic optimization using traffic cameras as sensors is driving the need to advance state-of-the-art multi-target multi-camera (MTMC) tracking. This work introduces CityFlow, a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in terms of spatial coverage and the number of cameras/videos in an urban environment. The dataset contains more than 200K annotated bounding boxes covering a wide range of scenes, viewing angles, vehicle models, and urban traffic flow conditions. Camera geometry and calibration information are provided to aid spatio-temporal analysis. In addition, a subset of the benchmark is made available for the task of image-based vehicle re-identification (ReID). We conducted an extensive experimental evaluation of baselines/state-of-the-art approaches in MTMC tracking, multi-target single-camera (MTSC) tracking, object detection, and image-based ReID on this dataset, analyzing the impact of different network architectures, loss functions, spatio-temporal models and their combinations on task effectiveness. An evaluation server is launched with the release of our benchmark at the 2019 AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to compare the performance of their newest techniques. We expect this dataset to catalyze research in this field, propel the state-of-the-art forward, and lead to deployed traffic optimization(s) in the real world.},
archivePrefix = {arXiv},
arxivId = {1903.09254},
author = {Tang, Zheng and Naphade, Milind and Liu, Ming Yu and Yang, Xiaodong and Birchfield, Stan and Wang, Shuo and Kumar, Ratnesh and Anastasiu, David and Hwang, Jenq Neng},
doi = {10.1109/CVPR.2019.00900},
eprint = {1903.09254},
file = {:home/bruno/Documentos/Mestrado/Tang{\_}CityFlow{\_}A{\_}City-Scale{\_}Benchmark{\_}for{\_}Multi-Target{\_}Multi-Camera{\_}Vehicle{\_}Tracking{\_}and{\_}CVPR{\_}2019{\_}paper.pdf:pdf},
isbn = {9781728132938},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Big Data,Datasets and Evaluation,Large Scale Methods,Motion and Tracking},
pages = {8789--8798},
title = {{Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification}},
volume = {2019-June},
year = {2019}
}
@inproceedings{Brandao2019,
abstract = {{\textcopyright} 2019 IEEE. Educational Data Mining (EDM) is the research area concerned with the use of data mining methods and its development in order to explore data sets collected in educational platforms. EDM has a considerable potential to improve the education quality. In this paper, we first model the Moodle variables of the Brazilian National School of Public Administration (Enap) database. Next we apply EDM to understand the standard Moodle variables influence and the proposed variables in the attendees performance and dropout rates. Our framework includes the following data mining approaches: decision tree, Adaboost based decision tree, random forest and k-means. The obtained Adaboost based decision tree outperforms the other approaches and the state-of-the-art framework with 91 {\%} of Precision, 89 {\%} of Recall and 90 {\%} of F1 score in terms of the attendees performance evaluation.},
author = {Brandao, I.V. and {Da Costa}, J.P.C.L. and Santos, G.A. and Praciano, B.J.G. and Junior, F.C.M.D. and {De S. Junior}, R.T.},
booktitle = {WCNPS 2019 - Workshop on Communication Networks and Power Systems},
doi = {10.1109/WCNPS.2019.8896312},
isbn = {9781728129204},
keywords = {Adaboost,Classification,Clustering,Decision tree,Educational Data Mining,Predictive analysis},
title = {{Classification and predictive analysis of educational data to improve the quality of distance learning courses}},
year = {2019}
}
@article{Simon2019,
abstract = {Lidar based 3D object detection is inevitable for autonomous driving, because it directly links to environmental understanding and therefore builds the base for prediction and motion planning. The capacity of inferencing highly sparse 3D data in real-time is an ill-posed problem for lots of other application areas besides automated vehicles, e.g. augmented reality, personal robotics or industrial automation. We introduce Complex-YOLO, a state of the art real-time 3D object detection network on point clouds only. In this work, we describe a network that expands YOLOv2, a fast 2D standard object detector for RGB images, by a specific complex regression strategy to estimate multi-class 3D boxes in Cartesian space. Thus, we propose a specific Euler-Region-Proposal Network (E-RPN) to estimate the pose of the object by adding an imaginary and a real fraction to the regression network. This ends up in a closed complex space and avoids singularities, which occur by single angle estimations. The E-RPN supports to generalize well during training. Our experiments on the KITTI benchmark suite show that we outperform current leading methods for 3D object detection specifically in terms of efficiency. We achieve state of the art results for cars, pedestrians and cyclists by being more than five times faster than the fastest competitor. Further, our model is capable of estimating all eight KITTI-classes, including Vans, Trucks or sitting pedestrians simultaneously with high accuracy.},
author = {Simon, Martin and Milz, Stefan and Amende, Karl and Gross, Horst Michael},
doi = {10.1007/978-3-030-11009-3_11},
file = {:home/bruno/Documentos/Mestrado/Simony{\_}Complex-YOLO{\_}An{\_}Euler-Region-Proposal{\_}for{\_}Real-time{\_}3D{\_}Object{\_}Detection{\_}on{\_}Point{\_}ECCVW{\_}2018{\_}paper.pdf:pdf},
isbn = {9783030110086},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {3D object detection,Autonomous driving,Lidar,Point cloud processing},
pages = {197--209},
title = {{Complex-YOLO: An euler-region-proposal for real-time 3D object detection on point clouds}},
volume = {11129 LNCS},
year = {2019}
}
@article{cutler2015many,
  title={How Many American Cities Are Preparing for the Arrival of Self-Driving Cars? Not Many},
  author={Cutler, Kim-Mai},
  journal={Techcrunch. com, http://bit. ly/CitiesUnprepared},
  volume={18},
  year={2015}
}

@incollection{world2004world,
  title={World report on road traffic injury prevention},
  author={World Health Organization and others},
  booktitle={World report on road traffic injury prevention},
  pages={217--217},
  year={2015}
}

@article{schoning2006parklenkassistent,
  title={Der Parklenkassistent" Park Assist" von Volkswagen/The Volkswagen" Park Assist"},
  author={Sch{\"o}ning, V and Katzwinkel, R and Wuttke, U and Schwitters, F and Rohlfs, M and Schuler, T},
  journal={VDI-Berichte},
  number={1960},
  year={2006}
}


@inproceedings{krasner2016automatic,
  title={Automatic parking identification and vehicle guidance with road awareness},
  author={Krasner, Guy and Katz, Eyal},
  booktitle={2016 IEEE International Conference on the Science of Electrical Engineering (ICSEE)},
  pages={1--5},
  year={2016},
  organization={IEEE}
}


@article{national2013preliminary,
  title={Preliminary statement of policy concerning automated vehicles},
  author={National Highway Traffic Safety Administration and others},
  journal={Washington, DC},
  pages={1--14},
  year={2013}
}


@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013}
}


@article{tzutalin6labelimg,
  title={LabelImg (2015)},
  author={Tzutalin, D},
  journal={GitHub repository https://github. com/tzutalin/labelImg},
  volume={6}
}


@inproceedings{mayer2016large,
  title={A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation},
  author={Mayer, Nikolaus and Ilg, Eddy and Hausser, Philip and Fischer, Philipp and Cremers, Daniel and Dosovitskiy, Alexey and Brox, Thomas},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4040--4048},
  year={2016}
}


@article{cao2013circle,
  title={Circle marker based distance measurement using a single camera},
  author={Cao, Yu-Tao and Wang, Jian-Ming and Sun, Yu-Kuan and Duan, Xiao-Jie},
  journal={Lecture Notes on Software Engineering},
  volume={1},
  number={4},
  pages={376},
  year={2013},
  publisher={IACSIT Press}
}


@misc{redmon2013darknet,
  title={Darknet: Open source neural networks in c},
  author={Redmon, Joseph},
  year={2013}
}


@article{krasin2017openimages,
  title={Openimages: A public dataset for large-scale multi-label and multi-class image classification},
  author={Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Ferrari, Vittorio and Abu-El-Haija, Sami and Kuznetsova, Alina and Rom, Hassan and Uijlings, Jasper and Popov, Stefan and Veit, Andreas and others},
  journal={Dataset available from https://github. com/openimages},
  volume={2},
  number={3},
  pages={2--3},
  year={2017}
}

@inproceedings{redmon2017yolo9000,
  title={YOLO9000: better, faster, stronger},
  author={Redmon, Joseph and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7263--7271},
  year={2017}
}

@article{ess2010object,
  title={Object detection and tracking for autonomous navigation in dynamic environments},
  author={Ess, Andreas and Schindler, Konrad and Leibe, Bastian and Van Gool, Luc},
  journal={The International Journal of Robotics Research},
  volume={29},
  number={14},
  pages={1707--1725},
  year={2010},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}


@article{Simon2019a,
abstract = {Accurate detection of 3D objects is a fundamental problem in computer vision and has an enormous impact on autonomous cars, augmented/virtual reality and many applications in robotics. In this work we present a novel fusion of neural network based state-of-the-art 3D detector and visual semantic segmentation in the context of autonomous driving. Additionally, we introduce Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable evaluation metric for comparison of object detections, which speeds up our inference time up to 20$\backslash${\%} and halves training time. On top, we apply state-of-the-art online multi target feature tracking on the object measurements to further increase accuracy and robustness utilizing temporal information. Our experiments on KITTI show that we achieve same results as state-of-the-art in all related categories, while maintaining the performance and accuracy trade-off and still run in real-time. Furthermore, our model is the first one that fuses visual semantic with 3D object detection.},
archivePrefix = {arXiv},
arxivId = {1904.07537},
author = {Simon, Martin and Amende, Karl and Kraus, Andrea and Honer, Jens and S{\"{a}}mann, Timo and Kaulbersch, Hauke and Milz, Stefan and Gross, Horst Michael},
eprint = {1904.07537},
file = {:home/bruno/Documentos/Mestrado/Simon{\_}Complexer-YOLO{\_}Real-Time{\_}3D{\_}Object{\_}Detection{\_}and{\_}Tracking{\_}on{\_}Semantic{\_}Point{\_}CVPRW{\_}2019{\_}paper.pdf:pdf},
title = {{Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic Point Clouds}},
url = {http://arxiv.org/abs/1904.07537},
year = {2019}
}
@article{Mariani2020,
abstract = {In the near future, our streets will be populated by myriads of autonomous self-driving vehicles to serve our diverse mobility needs. This will raise the need to coordinate their movements in order to properly handle both access to shared resources (e.g., intersections and parking slots) and the execution of mobility tasks (e.g., platooning and ramp merging). In this paper, we firstly introduce the general issues associated to coordination of autonomous vehicles, by identifying and framing the key classes of coordination problems. Following, we overview the different approaches that can be adopted to manage such coordination problems, by classifying them in terms of the degree of autonomy in decision making that is left to autonomous vehicles during coordination. Finally, we overview some further peculiar challenges that research will have to address before autonomously coordinated vehicles can safely hit our streets.},
archivePrefix = {arXiv},
arxivId = {2001.02443},
author = {Mariani, Stefano and Cabri, Giacomo and Zambonelli, Franco},
eprint = {2001.02443},
file = {:home/bruno/Downloads/2001.02443.pdf:pdf},
title = {{Coordination of Autonomous Vehicles: Taxonomy and Survey}},
url = {http://arxiv.org/abs/2001.02443},
year = {2020}
}
@article{Liu2020,
abstract = {Object detection, one of the most fundamental and challenging problems in computer vision, seeks to locate object instances from a large number of predefined categories in natural images. Deep learning techniques have emerged as a powerful strategy for learning feature representations directly from data and have led to remarkable breakthroughs in the field of generic object detection. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of the recent achievements in this field brought about by deep learning techniques. More than 300 research contributions are included in this survey, covering many aspects of generic object detection: detection frameworks, object feature representation, object proposal generation, context modeling, training strategies, and evaluation metrics. We finish the survey by identifying promising directions for future research.},
archivePrefix = {arXiv},
arxivId = {1809.02165},
author = {Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietik{\"{a}}inen, Matti},
doi = {10.1007/s11263-019-01247-4},
eprint = {1809.02165},
file = {:home/bruno/Downloads/Liu2020{\_}Article{\_}DeepLearningForGenericObjectDe.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Convolutional neural networks,Deep learning,Object detection,Object recognition},
number = {2},
pages = {261--318},
publisher = {Springer US},
title = {{Deep Learning for Generic Object Detection: A Survey}},
url = {https://doi.org/10.1007/s11263-019-01247-4},
volume = {128},
year = {2020}
}
@article{Unlu2019,
abstract = {Commercial Unmanned aerial vehicle (UAV) industry, which is publicly known as drone, has seen a tremendous increase in last few years, making these devices highly accessible to public. This phenomenon has immediately raised security concerns due to fact that these devices can intentionally or unintentionally cause serious hazards. In order to protect critical locations, the academia and industry have proposed several solutions in recent years. Computer vision is extensively used to detect drones autonomously compared to other proposed solutions such as RADAR, acoustics and RF signal analysis thanks to its robustness. Among these computer vision-based approaches, we see the preference of deep learning algorithms thanks to their effectiveness. In this paper, we are presenting an autonomous drone detection and tracking system which uses a static wide-angle camera and a lower-angle camera mounted on a rotating turret. In order to use memory and time efficiently, we propose a combined multi-frame deep learning detection technique, where the frame coming from the zoomed camera on the turret is overlaid on the wide-angle static camera's frame. With this approach, we are able to build an efficient pipeline where the initial detection of small sized aerial intruders on the main image plane and their detection on the zoomed image plane is performed simultaneously, minimizing the cost of resource exhaustive detection algorithm. In addition to this, we present the integral system including tracking algorithms, deep learning classification architectures and the protocols.},
author = {Unlu, Eren and Zenou, Emmanuel and Riviere, Nicolas and Dupouy, Paul Edouard},
doi = {10.1186/s41074-019-0059-x},
file = {:home/bruno/Documentos/Mestrado/Unlu2019{\_}Article{\_}DeepLearning-basedStrategiesFo.pdf:pdf},
issn = {18826695},
journal = {IPSJ Transactions on Computer Vision and Applications},
keywords = {Deep learning,Surveillance,Unmanned aerial vehicle},
number = {1},
publisher = {IPSJ Transactions on Computer Vision and Applications},
title = {{Deep learning-based strategies for the detection and tracking of drones using several cameras}},
volume = {11},
year = {2019}
}
@article{Wongsaree2018,
abstract = {This paper presents a distance determination technique using an image from the single forward camera. Since too dark or too bright of the image and non-linear relation between height of the object and distance from the camera have effect on the performance of the detection process. Therefore, automatic brightness adjustment and inverse perspective mapping (IMP) is applied in the proposed scheme. In addition, region of interest (ROI) determination is used to decrease the processing time. The experimental results confirm that the proposed technique can detect distance of the object in front of the car where the error is 7.96{\%}.},
author = {Wongsaree, Preaw and Sinchai, Sakkarin and Wardkein, Paramote and Koseeyaporn, Jeerasuda},
doi = {10.1109/CCOMS.2018.8463318},
file = {:home/bruno/Documentos/Mestrado/importante.pdf:pdf},
isbn = {9781538663509},
journal = {2018 3rd International Conference on Computer and Communication Systems, ICCCS 2018},
keywords = {component,distance detection,gamma correction,inverse perspective mapping,opencv},
pages = {323--326},
publisher = {IEEE},
title = {{Distance Detection Technique Using Enhancing Inverse Perspective Mapping}},
year = {2018}
}
@article{Tuohy2010,
abstract = {This paper presents a novel real-time distance determination algorithm using an image sensor for use in an automobile environment. The system uses a forward facing camera placed within the vehicle. From a single forward facing image, it is difficult to determine distances to objects in front of the vehicle with any degree of certainty. There is a non linear relationship between the height of an object in a forward facing image and its distance from the camera. This paper presents a method which uses Inverse Perspective Mapping (IPM) to overcome this problem. Using IPM, we can transform the forward facing image to a top-down "bird's eye" view, in which there is a linear relationship between distances in the image and in the real world. The algorithm is implemented in the C language using the OpenCV libraries. Implementation in OpenCV leads to a high performance, low overhead system that could be implemented on a low power embedded device in an automotive environment.},
author = {Tuohy, S. and O'Cualain, D. and Jones, E. and Glavin, M.},
doi = {10.1049/cp.2010.0495},
file = {:home/bruno/Documentos/Mestrado/docualain{\_}issc10 (1).pdf:pdf},
isbn = {9781849192521},
journal = {IET Conference Publications},
keywords = {Distance Detection,Inverse Perspective Mapping,OpenCV},
number = {566 CP},
pages = {100--105},
title = {{Distance determination for an automobile environment using inverse perspective mapping in OpenCV}},
volume = {2010},
year = {2010}
}
@article{Ali2016,
abstract = {This paper describes systematically two methods used in intelligent transportation systems: Distance Estimation using an onboard camera and car position detection. Distance estimation is a method for detecting distance for the preceding vehicles based on monocular camera. Vehicle position detection is a method of specifying the vehicle position relative to the road that can serve as Lane Departure Warning system. These two approaches have been discussed and implemented in this article. For lane detection and tracking, Hough Transform and Kalman filter were adopted. A brief introduction about both lane detection system and object detection is given. Finally, both approaches have been evaluated on a large dataset of videos.},
author = {Ali, Abduladhem Abdulkareem and Hussein, Hussein Alaa},
doi = {10.1109/AIC-MITCSA.2016.7759904},
file = {:home/bruno/Documentos/Mestrado/07759904.pdf:pdf},
isbn = {9781509032471},
journal = {Al-Sadiq International Conference on Multidisciplinary in IT and Communication Techniques Science and Applications, AIC-MITCSA 2016},
keywords = {Distance estimation,driver assistance systems,lane departure warning system,onboard vehicular camera,vehicle position},
number = {1},
pages = {20--23},
publisher = {IEEE},
title = {{Distance estimation and vehicle position detection based on monocular camera}},
year = {2016}
}
@article{Qi2019,
abstract = {In order to measure the distance between our vehicle and the target vehicle by monocular vision and eliminate the estimation error bring by changing of vehicle pose, we propose the distance estimation method based on the vehicle pose information, which can be used to eliminate the error of distance estimation effectively cause by the change of the pitch angle and roll angle of unmanned vehicle. In addition, the pose information could also help us estimate whether the vehicle is in a slope, thus engage in distance estimation for the vehicles. Several groups of data was collected for the experiments. And the result prove the validity of the algorithm in distance estimation.},
author = {Qi, S. H. and Li, J. and Sun, Z. P. and Zhang, J. T. and Sun, Y.},
doi = {10.1088/1742-6596/1168/3/032040},
file = {:home/bruno/Documentos/Mestrado/Qi{\_}2019{\_}J.{\_}Phys.{\_}{\_}Conf.{\_}Ser.{\_}1168{\_}032040.pdf:pdf},
issn = {17426596},
journal = {Journal of Physics: Conference Series},
number = {3},
title = {{Distance Estimation of Monocular Based on Vehicle Pose Information}},
volume = {1168},
year = {2019}
}
@article{Salman2017,
abstract = {Self-driving cars reduce human error and can accomplish various missions to help people in different fields. They have become one of the main interests in automotive research and development, both in the industry and academia. However, many challenges are encountered in dealing with distance measurement and cost, both in equipment and technique. The use of stereo camera to measure the distance of an object is convenient and popular for obstacle avoidance and navigation of autonomous vehicles. The calculation of distance considers angular distance, distance between cameras, and the pixel of the image. This study proposes a method that measures object distance based on trigonometry, that is, facing the self-driving car using image processing and stereo vision with high accuracy, low cost, and computational speed. The method achieves a high distance measuring accuracy of up to 20 m. It can be implemented in real time computing systems and can determine the safe driving distance between obstacles.},
author = {Salman, Yasir Dawood and Ku-mahamud, Ku Ruhana and Kamioka, Eiji},
file = {:home/bruno/Documentos/Mestrado/PID105-235-242e.pdf:pdf},
journal = {Proceeding of the 6Th International Conference of Computing {\&} Informations},
keywords = {distance measurement,image,self-driving car,stereo camera},
number = {105},
pages = {235--242},
title = {{Distance Measurement for Self-Driving Cars Using Stereo Camera}},
year = {2017}
}
@article{Salman2017a,
abstract = {Self-driving cars reduce human error and can accomplish various missions to help people in different fields. They have become one of the main interests in automotive research and development, both in the industry and academia. However, many challenges are encountered in dealing with distance measurement and cost, both in equipment and technique. The use of stereo camera to measure the distance of an object is convenient and popular for obstacle avoidance and navigation of autonomous vehicles. The calculation of distance considers angular distance, distance between cameras, and the pixel of the image. This study proposes a method that measures object distance based on trigonometry, that is, facing the self-driving car using image processing and stereo vision with high accuracy, low cost, and computational speed. The method achieves a high distance measuring accuracy of up to 20 m. It can be implemented in real time computing systems and can determine the safe driving distance between obstacles.},
author = {Salman, Yasir Dawood and Ku-mahamud, Ku Ruhana and Kamioka, Eiji},
file = {:home/bruno/Documentos/Mestrado/PID105-235-242e (1).pdf:pdf},
journal = {Proceeding of the 6Th International Conference of Computing {\&} Informations},
keywords = {distance measurement,image,self-driving car,stereo camera},
number = {105},
pages = {235--242},
title = {{Distance Measurement for Self-Driving Cars Using Stereo Camera}},
year = {2017}
}

@article{stevenson2011long,
  title={Long-distance car radar},
  author={Stevenson, Richard},
  journal={IEEE Spectrum},
  note = {Available at \url{https://spectrum.ieee.org/transportation/advanced-cars/longditance-car-radar}, accessed at 17 June 2020},
  year={2011}
}

@inproceedings{yang2020feedback,
  title={Feedback Recurrent AutoEncoder},
  author={Yang, Yang and Sauti{\`e}re, Guillaume and Ryu, J Jon and Cohen, Taco S},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3347--3351},
  year={2020},
  organization={IEEE}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}

@article{cambridge2009introduction,
  title={Introduction to information retrieval},
  author={Cambridge, UP},
  year={2009}
}


@article{Halko_2011,
   title={Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions},
   volume={53},
   ISSN={1095-7200},
   url={http://dx.doi.org/10.1137/090771806},
   DOI={10.1137/090771806},
   number={2},
   journal={SIAM Review},
   publisher={Society for Industrial & Applied Mathematics (SIAM)},
   author={Halko, N. and Martinsson, P. G. and Tropp, J. A.},
   year={2011},
   month={Jan},
   pages={217–288}
}


@inproceedings{liu2004distance,
  title={Distance based kernel PCA image reconstruction},
  author={Liu, Qingshan and Cheng, Jian and Lu, Hanqing and Ma, Songde},
  booktitle={Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},
  volume={3},
  pages={670--673},
  year={2004},
  organization={IEEE}
}



@inproceedings{wang2019data,
  title={A data driven method of feedforward compensator optimization for autonomous vehicle control},
  author={Wang, Pin and Shi, Tianyu and Zou, Chonghao and Xin, Long and Chan, Ching-Yao},
  booktitle={2019 IEEE Intelligent Vehicles Symposium (IV)},
  pages={2012--2017},
  year={2019},
  organization={IEEE}
}

@book{forsyth2002computer,
  title={Computer vision: a modern approach},
  author={Forsyth, David A and Ponce, Jean},
  year={2002},
  publisher={Prentice Hall Professional Technical Reference}
}

@article{zhu2020camera,
  title={Camera calibration from very few images based on soft constraint optimization},
  author={Zhu, Hongjun and Li, Yan and Liu, Xin and Yin, Xuehui and Shao, Yanhua and Qian, Ying and Tan, Jindong},
  journal={Journal of the Franklin Institute},
  volume={357},
  number={4},
  pages={2561--2584},
  year={2020},
  publisher={Elsevier}
}

@article{bonnefon2016social,
  title={The social dilemma of autonomous vehicles},
  author={Bonnefon, Jean-Fran{\c{c}}ois and Shariff, Azim and Rahwan, Iyad},
  journal={Science},
  volume={352},
  number={6293},
  pages={1573--1576},
  year={2016},
  publisher={American Association for the Advancement of Science}
}


@article{gao2018object,
  title={Object classification using CNN-based fusion of vision and LIDAR in autonomous vehicle environment},
  author={Gao, Hongbo and Cheng, Bo and Wang, Jianqiang and Li, Keqiang and Zhao, Jianhui and Li, Deyi},
  journal={IEEE Transactions on Industrial Informatics},
  volume={14},
  number={9},
  pages={4224--4231},
  year={2018},
  publisher={IEEE}
}


@article{kamerad,
  title={Radar sensor module to bring added safety to autonomous driving},
  author={Fraunhofer Institute},
  journal={Fraunhofer Institute},
  note = {Available at \url{https://www.fraunhofer.de/en/press/research-news/2019/june/radar-sensor-module-to-bring-added-safety-to-autonomous-driving.html}, accessed at 17 June 2020},
  year={2019}
}

@misc{ariyur2006collision,
  title={Collision avoidance involving radar feedback},
  author={Ariyur, Kartik and Enns, Dale and Lommel, Peter},
  year={2006},
  month=mar # "~16",
  publisher={Google Patents},
  note={US Patent App. 10/941,535}
}


@ARTICLE{888718,
  author={Z. {Zhang}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A flexible new technique for camera calibration}, 
  year={2000},
  volume={22},
  number={11},
  pages={1330-1334},}
  
  
@article{Zaarane2020,
abstract = {The focus of this paper is inter-vehicles distance measurement which is a very important and challenging task in image processing domain. Where it is used in several systems such as Driving Safety Support Systems (DSSS), autonomous driving and traffic mobility. In the current paper, we propose an inter-vehicle distance measurement system for self-driving based on image processing. The proposed system uses two cameras mounted as one stereo camera, in the hosting vehicle behind the rear-view mirror. The detection of vehicles is performed first in a single camera using a recent powerful work from the literature. Then, the same vehicle is detected in the image captured by the second camera using template matching technique. Thus, the inter-vehicle distance is calculated using a simple method based on the position of the vehicle in both cameras, geometric derivations and additional technical data such as distance between the cameras and some other specific angles (e.g. the cameras view field angle). The results of the extensive experiments showed the high accuracy of the proposed method compared to the previous works from literature and it allows to measure efficiently the distances between the vehicles and the hosting vehicle. In addition, this method could be used in several systems of various domains in real time regardless of the object types. The experiments results were done on a Hardware Processor System (HPS) located in a VEEK-MT2S provided by TERASIC.},
author = {Zaarane, Abdelmoghit and Slimani, Ibtissam and {Al Okaishi}, Wahban and Atouf, Issam and Hamdoun, Abdellatif},
doi = {10.1016/j.array.2020.100016},
file = {:home/bruno/Documentos/Mestrado/1-s2.0-S2590005620300011-main.pdf:pdf},
issn = {25900056},
journal = {Array},
keywords = {distance measurement},
number = {May 2019},
pages = {100016},
publisher = {Elsevier Ltd},
title = {{Distance measurement system for autonomous vehicles using stereo camera}},
url = {https://doi.org/10.1016/j.array.2020.100016},
volume = {5},
year = {2020}
}
@article{Xu2020,
abstract = {Object detection has recently experienced substantial progress. Yet, the widely adopted horizontal bounding box representation is not appropriate for ubiquitous oriented objects such as objects in aerial images and scene texts. In this paper, we propose a simple yet effective framework to detect multi-oriented objects. Instead of directly regressing the four vertices, we glide the vertex of the horizontal bounding box on each corresponding side to accurately describe a multi-oriented object. Specifically, We regress four length ratios characterizing the relative gliding offset on each corresponding side. This may facilitate the offset learning and avoid the confusion issue of sequential label points for oriented objects. To further remedy the confusion issue for nearly horizontal objects, we also introduce an obliquity factor based on area ratio between the object and its horizontal bounding box, guiding the selection of horizontal or oriented detection for each object. We add these five extra target variables to the regression head of faster R-CNN, which requires ignorable extra computation time. Extensive experimental results demonstrate that without bells and whistles, the proposed method achieves superior performances on multiple multi-oriented object detection benchmarks including object detection in aerial images, scene text detection, pedestrian detection in fisheye images.},
archivePrefix = {arXiv},
arxivId = {1911.09358},
author = {Xu, Yongchao and Fu, Mingtao and Wang, Qimeng and Wang, Yukang and Chen, Kai and Xia, Gui-Song and Bai, Xiang},
doi = {10.1109/tpami.2020.2974745},
eprint = {1911.09358},
file = {:home/bruno/Downloads/09001201.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {XX},
pages = {1--1},
title = {{Gliding vertex on the horizontal bounding box for multi-oriented object detection}},
volume = {XX},
year = {2020}
}
@article{Pan2019,
abstract = {Global calibration of multi-vision sensors in the railway fields is easily affected by on-site complex environments, such as lighting and self-occlusion, which makes it difficult for existing methods to achieve high-Accuracy calibration. In this paper, a high-Accuracy and flexible calibration method of multi-vision sensors in the outdoor railway fields based on flexible and optimal 3D data field through the combination of articular arm and metal target embedded with luminescent LEDs is proposed. Firstly, a high-Accuracy and flexible 3D data field with multiple angles and views is constructed by the articular arm and the metal target rapidly; Secondly, the high-frequency multi-exposure imaging mode and multi-scale image feature points extraction are adopted, and the high-Accuracy reposition is achieved through Kalman filter, which can reduce the impact of image noise efficiently; Finally, the RANSAC method is utilized to optimize the 3D data field, forming the optimal 3D data field, and the optimal target chains corresponding to each group of cameras are established. The maximum likelihood solution of global parameters is solved by nonlinear optimization. Simulation experiments verify the feasibility of the proposed method. Meanwhile, physical experiment results show this method can reduce the outdoor environment impact and improve the calibration and measurement precision effectively. In addition, the computational efficiency of the proposed method is about 11.2 times than multi-images averaging method, the calibration and measurement accuracy improve about 20 and 2.9 times respectively, which is suitable for the high-Accuracy and flexible global calibration in the complex railway environment.},
author = {Pan, Xiao and Liu, Zhen and Zhang, Guangjun},
doi = {10.1109/ACCESS.2019.2950283},
file = {:home/bruno/Documentos/Mestrado/08886505.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {3D data field,Flexible,global calibration,high-Accuracy,multi-vision sensors,optimal},
pages = {159495--159506},
publisher = {IEEE},
title = {{High-Accuracy Calibration of On-Site Multi-Vision Sensors Based on Flexible and Optimal 3D Field}},
volume = {7},
year = {2019}
}
@inproceedings{DaSilva2019,
abstract = {{\textcopyright} 2019 Polish Information Processing Society - as since. Drivers' behavior in traffic is a determining factor for the rate of accidents on roads and highways. This paper presents the design of an intelligent IoT system capable of inferring and warning about road traffic risks and danger zones, based on data obtained from the vehicles and their drivers mobile phones, thus helping to avoid accidents and seeking to preserve the lives of the passengers. The proposed approach is to collect vehicle telemetry data and mobile phone sensors data through an IoT network and then to analyze the driver's behavior while driving, along with data from the environment. The results of the inference serve to alert drivers about incidents in their trajectory as well as to provide feedback on how they are driving. The proposal is validated using a developed prototype to test its data collection and inference features in a small scale experiment.},
author = {{Da Silva}, D.A. and Torres, J.A.S. and Pinheiro, A. and {De Caldas Filho}, F.L. and Mendonca, F.L.L. and Praciano, B.J.G. and {De Oliveira Kfouri}, G. and {De Sousa}, R.T.},
booktitle = {Proceedings of the 2019 Federated Conference on Computer Science and Information Systems, FedCSIS 2019},
doi = {10.15439/2019F263},
isbn = {9788395541605},
keywords = {Android,Driving behavior,Inference,Internet of Things,OBD-II,Vehicular networks},
title = {{Inference of driver behavior using correlated IoT data from the vehicle telemetry and the driver mobile phone}},
year = {2019}
}
@article{Sivaraman2013,
abstract = {In this paper, we introduce a synergistic approach to integrated lane and vehicle tracking for driver assistance. The approach presented in this paper results in a final system that improves on the performance of both lane tracking and vehicle tracking modules. Further, the presented approach introduces a novel approach to localizing and tracking other vehicles on the road with respect to lane position, which provides information on higher contextual relevance that neither the lane tracker nor vehicle tracker can provide by itself. Improvements in lane tracking and vehicle tracking have been extensively quantified. Integrated system performance has been validated on real-world highway data. Without specific hardware and software optimizations, the fully implemented system runs at near-real-time speeds of 11 frames per second. {\textcopyright} 2000-2011 IEEE.},
author = {Sivaraman, Sayanan and Trivedi, Mohan Manubhai},
doi = {10.1109/TITS.2013.2246835},
file = {:home/bruno/Documentos/Mestrado/SayananTrivedi{\_}IEEETITS-june2013.pdf:pdf},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Active safety,computer vision,driver assistance,intelligent vehicles,lane departure,lane tracking,vehicle tracking},
number = {2},
pages = {906--917},
title = {{Integrated lane and vehicle detection, localization, and tracking: A synergistic approach}},
volume = {14},
year = {2013}
}
@article{Skulimowski2012,
abstract = {This paper presents the theoretical foundations of an intelligent on-line modelling tool capable of processing heterogeneous information on complex techno-economical systems. Its main functionality is to investigate, elicit, and apply rules and principles that govern the development processes of technologies and related markets. Specifically, we will focus on applications of the tool to model the evolution of information technology (IT). We will distinguish several relevant subsystems of the system under study, which describe the demographic, education, global economic trends, as well as specific market factors that determine the demand for and use of IT. The group modelling techniques are implemented in the new tool to enable the collaborative and distributed model building with intelligent verification of entries called 'model wiki'. Based on the information elicited from experts, gathered from the web and professional databases, a discrete-time control model of technological evolution emerges, coupled with a controlled discrete-event system. The latter processes qualitative information and models the influence of external events and trends on the discrete-time control system parameters. We propose novel uncertainty handling techniques capable of processing and combining different types of uncertain information, coming i.a. from Delphi research and forecasts. The quantitative information is dynamically updated by autonomous webcrawlers, following an adaptive intelligent strategy. The resulting model can be used to simulate long-term future trends and scenarios. Its ultimate goal is to perform an optimization process and derive recommendations for decision makers, for example when selecting IT investment strategies in an innovative enterprise. {\textcopyright} 2012 Springer-Verlag.},
author = {Skulimowski, Andrzej M J},
doi = {10.1007/978-3-642-31919-8},
file = {:home/bruno/Documentos/Mestrado/Camera{\_}Array{\_}Synthetic{\_}Aperture{\_}Focusing{\_}and{\_}Fusio.pdf:pdf},
isbn = {978-3-642-31918-1},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Complex Systems,Decision Support Systems,Discrete-Time Control,Foresight,Group Modelling Tool,Hybrid Models,Model Discovery},
number = {February 2015},
pages = {614--626},
title = {{Intelligent Science and Intelligent Data Engineering}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84865808451{\&}partnerID=tZOtx3y1},
volume = {7202},
year = {2012}
}
@article{Mallot1991,
abstract = {We present a scheme for obstacle detection from optical flow which is based on strategies of biological information processing. Optical flow is established by a local "voting" (non-maximum suppression) over the outputs of correlation-type motion detectors similar to those found in the fly visual system. The computational theory of obstacle detection is discussed in terms of space-variances of the motion field. An efficient mechanism for the detection of disturbances in the expected motion field is based on "inverse perspective mapping", i.e., a coordinate transform or retinotopic mapping applied to the image. It turns out that besides obstacle detection, inverse perspective mapping has additional advantages for regularizing optical flow algorithms. Psychophysical evidence for body-scaled obstacle detection and related neurophysiological results are discussed. {\textcopyright} 1991 Springer-Verlag.},
author = {Mallot, Hanspeter A. and B{\"{u}}lthoff, H. H. and Little, J. J. and Bohrer, S.},
doi = {10.1007/BF00201978},
file = {:home/bruno/Documentos/Mestrado/MallotBulLitBoh{\_}BiolCyb91.pdf:pdf},
issn = {03401200},
journal = {Biological Cybernetics},
number = {3},
pages = {177--185},
title = {{Inverse perspective mapping simplifies optical flow computation and obstacle detection}},
volume = {64},
year = {1991}
}
@article{Wang2019,
abstract = {Aiming at the problem that the harvesting width measurement of rice or wheat is susceptible to interference and poor real-time performance, an improved probabilistic Hough transform (PHT) width measurement algorithm based on morphological vertical line extraction is proposed. Inverse perspective mapping (IPM) restores the parallel relationship of the vertical line of the three-dimensional world. Image enhancement of the object region is used to highlight differences between the harvested and unharvested region. The median filter of large size filter window eliminates the interference of local noise. Improved PHT based on the vertical line extraction of the threshold segmentation realizes the real-time accurate measurement of the harvesting width during the operation of the intelligent combine harvester. Experiments demonstrate that the improved PHT can accurately obtain the harvesting width error of not more than 64 mm at an average success rate of 96.025{\%}, and can measure the harvesting width in real time at 77.675 ms/frame.},
author = {Wang, Lihui and Yang, Yu and Shi, Jiachen},
doi = {10.1016/j.measurement.2019.107130},
file = {:home/bruno/Downloads/1-s2.0-S0263224119309960-main.pdf:pdf},
issn = {02632241},
journal = {Measurement: Journal of the International Measurement Confederation},
keywords = {Harvesting width measurement,Image processing,Improved probabilistic Hough transform,Intelligent combine harvester,Vision navigation},
pages = {107130},
publisher = {Elsevier Ltd},
title = {{Measurement of harvesting width of intelligent combine harvester by improved probabilistic Hough transform algorithm}},
url = {https://doi.org/10.1016/j.measurement.2019.107130},
volume = {151},
year = {2019}
}
@article{Huang2018,
author = {Huang, Liqin and Chen, Yanan and Fan, Zhengjia and Chen, Zhifeng},
doi = {10.1117/1.jei.27.4.043019},
file = {:home/bruno/Documentos/Mestrado/043019{\_}1.pdf:pdf},
isbn = {3425842463},
issn = {1560-229X},
journal = {Journal of Electronic Imaging},
keywords = {20,2018,24,28,absolute distance estimation,accepted for publication jun,deep learning,instance segmentation,monocular vision,paper 180230 received mar,published online jul,vehicle classification},
number = {04},
pages = {1},
title = {{Measuring the absolute distance of a front vehicle from an in-car camera based on monocular vision and instance segmentation}},
volume = {27},
year = {2018}
}
@article{Oliveira2015,
abstract = {Over the past years, inverse perspective mapping has been successfully applied to several problems in the field of Intelligent Transportation Systems. In brief, the method consists of mapping images to a new coordinate system where perspective effects are removed. The removal of perspective associated effects facilitates road and obstacle detection and also assists in free space estimation. There is, however, a significant limitation in the inverse perspective mapping: the presence of obstacles on the road disrupts the effectiveness of the mapping. The current paper proposes a robust solution based on the use of multimodal sensor fusion. Data from a laser range finder is fused with images from the cameras, so that the mapping is not computed in the regions where obstacles are present. As shown in the results, this considerably improves the effectiveness of the algorithm and reduces computation time when compared with the classical inverse perspective mapping. Furthermore, the proposed approach is also able to cope with several cameras with different lenses or image resolutions, as well as dynamic viewpoints.},
author = {Oliveira, Miguel and Santos, Vitor and Sappa, Angel D.},
doi = {10.1016/j.inffus.2014.09.003},
file = {:home/bruno/Documentos/Mestrado/J{\_}{\_}Elsevier{\_}IF{\_}Vol{\_}24{\_}July{\_}2015{\_}pp{\_}108-121.pdf:pdf},
issn = {15662535},
journal = {Information Fusion},
keywords = {Intelligent vehicles,Inverse perspective mapping,Multimodal sensor fusion},
pages = {108--121},
publisher = {Elsevier B.V.},
title = {{Multimodal inverse perspective mapping}},
url = {http://dx.doi.org/10.1016/j.inffus.2014.09.003},
volume = {24},
year = {2015}
}
@article{Rangesh2019,
abstract = {Online multi-object tracking (MOT) is extremely important for high-level spatial reasoning and path planning for autonomous and highly-automated vehicles. In this paper, we present a modular framework for tracking multiple objects (vehicles), capable of accepting object proposals from different sensor modalities (vision and range) and a variable number of sensors, to produce continuous object tracks. This work is a generalization of the MDP framework for MOT, with some key extensions - First, we track objects across multiple cameras and across different sensor modalities. This is done by fusing object proposals across sensors accurately and efficiently. Second, the objects of interest (targets) are tracked directly in the real world. This is a departure from traditional techniques where objects are simply tracked in the image plane. Doing so allows the tracks to be readily used by an autonomous agent for navigation and related tasks. To verify the effectiveness of our approach, we test it on real world highway data collected from a heavily sensorized testbed capable of capturing full-surround information. We demonstrate that our framework is well-suited to track objects through entire maneuvers around the ego-vehicle, some of which take more than a few minutes to complete. We also leverage the modularity of our approach by comparing the effects of including/excluding different sensors, changing the total number of sensors, and the quality of object proposals on the final tracking result.},
archivePrefix = {arXiv},
arxivId = {1802.08755},
author = {Rangesh, Akshay and Trivedi, Mohan Manubhai},
doi = {10.1109/tiv.2019.2938110},
eprint = {1802.08755},
file = {:home/bruno/Documentos/Mestrado/1802.08755.pdf:pdf},
issn = {2379-8858},
journal = {IEEE Transactions on Intelligent Vehicles},
number = {4},
pages = {588--599},
title = {{No Blind Spots: Full-Surround Multi-Object Tracking for Autonomous Vehicles Using Cameras and LiDARs}},
volume = {4},
year = {2019}
}
@article{Hofmann2019,
abstract = {In the future, autonomously driving vehicles have to navigate in challenging environments. In some situations, their perception capabilities are not able to generate a reliable overview of the environment, by reason of occlusions. In this contribution, an infrastructural stereo camera system for environment perception is proposed. Similar existing systems only detect moving objects by background subtraction algorithms and monocular cameras. In contrast, the proposed approach fuses three different algorithms for object detection and classification and uses stereo vision for object localization. The algorithmic concept is composed of a background subtraction algorithm based on Gaussian Mixture Models, the convolutional neural network”You only look once” as well as a novel algorithm for detecting salient objects in depth maps. The combination of these complementary object detection principles allows the reliable detection of dynamic as well as static objects. An algorithm for fusing the results of the three object detection methods based on bounding boxes is introduced. The proposed fusion algorithm for bounding boxes improves the detection results and provides an information fusion. We evaluate the proposed concept on real word data. The object detection, classification and localization in the real world scenario is investigated and discussed.},
author = {Hofmann, Christian and Particke, Florian and Hiller, Markus and Thielecke, J{\"{o}}rn},
doi = {10.5220/0007370408080815},
file = {:home/bruno/Documentos/Mestrado/VISAPP{\_}2019{\_}104{\_}CR.pdf:pdf},
isbn = {9789897583544},
journal = {VISIGRAPP 2019 - Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
keywords = {Autonomous Driving,Deep Learning,Infrastructural Cameras,Object Detection,Robotics,Stereo Vision},
pages = {808--815},
title = {{Object detection, classification and localization by infrastructural stereo cameras}},
volume = {5},
year = {2019}
}
@article{Sankaranarayanan2008,
abstract = {Video cameras are among the most commonly used sensors in a large number of applications, ranging from surveillance to smart rooms for videoconferencing. There is a need to develop algorithms for tasks such as detection, tracking, and recognition of objects, specifically using distributed networks of cameras. The projective nature of imaging sensors provides ample challenges for data association across cameras. We first discuss the nature of these challenges in the context of visual sensor networks. Then, we show how real-world constraints can be favorably exploited in order to tackle these challenges. Examples of real-world constraints are a) the presence of a world plane, b) the presence of a three-dimiensional scene model, c) consistency of motion across cameras, and d) color and texture properties. In this regard, the main focus of this paper is towards highlightingthe efficient use of the geometric constraints induced by the imaging devices to derive distributed algorithms for target detection, tracking, and recognition. Our discussions are supported by several examples drawn from real applications. Lastly, we also describe several potential research problems that remain to be addressed. {\textcopyright} 2008 IEEE.},
author = {Sankaranarayanan, Aswin C. and Veeraraghavan, Ashok and Chellappa, Rama},
doi = {10.1109/JPROC.2008.928758},
file = {:home/bruno/Documentos/Mestrado/Sank{\_}PIEEE{\_}2008.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Detection,Distributed sensing,Geometric constraints,Multiview geometry,Recognition,Smart cameras,Tracking},
number = {10},
pages = {1606--1624},
title = {{Object detection, tracking and recognition for multiple smart cameras}},
volume = {96},
year = {2008}
}
@article{Song2013,
abstract = {The object position measuring is an important work of dual-view stereo vision. At present, the most widely used dual-view stereo method of measurement is based on the binocular disparity and geometry between the left and right images. In this paper, we proposed two object position measuring methods based on an adjustable dual-view camera, it requires the optical axes are converged to the same point on the target object. The first method of object position measuring is based on the rotation angles of the two cameras and the pitching angle of the optical axial plane. The second method of object position measuring is based on the relative extrinsic parameters. The proposed methods can perform good position measuring without the epipolar rectification and stereo matching. The validities and reliabilities of the proposed methods are verified by the experimental results. {\textcopyright} 2013 IEEE.},
author = {Song, Xiaowei and Wu, Yuanzhao and Yang, Lei and Liu, Zhong},
doi = {10.1109/ICMEW.2013.6618433},
file = {:home/bruno/Documentos/Mestrado/06618433.pdf:pdf},
isbn = {9781479916047},
journal = {Electronic Proceedings of the 2013 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2013},
keywords = {convergent point,dual-view stereo camera,object position measuring,relative extrinsic parameters},
pages = {1--6},
publisher = {IEEE},
title = {{Object position measuring based on adjustable dual-view camera}},
year = {2013}
}
@article{Mino2016,
author = {Mino, Atsushi},
file = {:home/bruno/Documentos/Mestrado/42-7.pdf:pdf},
pages = {47--51},
title = {{Obstacle Detection by Monocular Camera}},
year = {2016}
}
@article{Pei2019,
abstract = {With the three-dimensional (3D) coordinates of objects captured by a sequence of images taken in different views, object reconstruction is a technique which aims to recover the shape and appearance information of objects. Although great progress in object reconstruction has been made over the past few years, object reconstruction in occlusion situations remains a challenging problem. In this paper, we propose a novel method to reconstruct occluded objects based on synthetic aperture imaging. Unlike most existing methods, which either assume that there is no occlusion in the scene or remove the occlusion from the reconstructed result, our method uses the characteristics of synthetic aperture imaging that can effectively reduce the influence of occlusion to reconstruct the scene with occlusion. The proposed method labels occlusion pixels according to variance and reconstructs the 3D point cloud based on synthetic aperture imaging. Accuracies of the point cloud are tested by calculating the spatial difference between occlusion and non-occlusion conditions. The experiment results show that the proposed method can handle the occluded situation well and demonstrates a promising performance.},
author = {Pei, Zhao and Li, Yawen and Ma, Miao and Li, Jun and Leng, Chengcai and Zhang, Xiaoqiang and Zhang, Yanning},
doi = {10.3390/s19030607},
file = {:home/bruno/Documentos/Mestrado/sensors-19-00607.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Occluded-object 3D reconstruction,Synthetic aperture imaging},
number = {3},
pages = {1--22},
title = {{Occluded-object 3D reconstruction using camera array synthetic aperture imaging}},
volume = {19},
year = {2019}
}
@inproceedings{DeMendonca2019,
abstract = {The general structure of the Internet of Things (IoT) networks is still an interesting matter for research and innovation. Common IoT uses scenarios show an undefined number of IoT instances running independently and simultaneously. In general, each IoT instance operates under a middleware that provides a set of abstractions to facilitate the integration and communication of components, acting as a mediation layer between the physical layer and the application layer. This paper proposes a model for the interoperation of different IoT instances using peer-to-peer (P2P) services providing operations to federate these instances in fixed and mobile scenarios. The proposed architecture supports services to merge and split IoT instances, as well as to move, replicate and delete IoT data, software entities, and devices's abstractions. The proposal is validated by testing a developed prototype and discussing its functionalities and performance.},
author = {{De Mendonca}, Fabio L.L. and {Da Cunha}, Dayanne F. and Praciano, Bruno J.G. and {Da Rosa Zanatta}, Mateus and {Da Costa}, Joao Paulo C.L. and {De Sousa}, Rafael T.},
booktitle = {WCNPS 2019 - Workshop on Communication Networks and Power Systems},
doi = {10.1109/WCNPS.2019.8896313},
isbn = {9781728129204},
keywords = {Federated IoT,Internet of Things (IoT),IoT middleware,Peer-to-peer (P2P) services},
title = {{P2PIoT: A peer-to-peer communication model for the internet of things}},
year = {2019}
}
@article{Ghobadi2010,
author = {Ghobadi, Seyed Eghbal},
file = {:home/bruno/Documentos/Mestrado/56725747.pdf:pdf},
pages = {141},
title = {{Real Time Object Recognition and Tracking Using 2D/3D Images}},
year = {2010}
}
@article{Cui2019,
abstract = {We present a real-time dense geometric mapping algorithm for large-scale environments. Unlike existing methods which use pinhole cameras, our implementation is based on fisheye cameras whose large field of view benefits various computer vision applications for self-driving vehicles such as visual-inertial odometry, visual localization, and object detection. Our algorithm runs on in-vehicle PCs at approximately 15 Hz, enabling vision-only 3D scene perception for self-driving vehicles. For each synchronized set of images captured by multiple cameras, we first compute a depth map for a reference camera using plane-sweeping stereo. To maintain both accuracy and efficiency, while accounting for the fact that fisheye images have a lower angular resolution, we recover the depths using multiple image resolutions. We adopt the fast object detection framework, YOLOv3, to remove potentially dynamic objects. At the end of the pipeline, we fuse the fisheye depth images into the truncated signed distance function (TSDF) volume to obtain a 3D map. We evaluate our method on large-scale urban datasets, and results show that our method works well in complex dynamic environments.},
archivePrefix = {arXiv},
arxivId = {1809.06132},
author = {Cui, Zhaopeng and Heng, Lionel and Yeo, Ye Chuan and Geiger, Andreas and Pollefeys, Marc and Sattler, Torsten},
doi = {10.1109/ICRA.2019.8793884},
eprint = {1809.06132},
file = {:home/bruno/Documentos/Mestrado/1809.06132.pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {6087--6093},
title = {{Real-time dense mapping for self-driving vehicles using fisheye cameras}},
volume = {2019-May},
year = {2019}
}
@article{Ramesh2018,
abstract = {The aim of this research is to design an intelligent system that addresses the problem of real-time localization and navigation of visually impaired (VI) in an indoor environment using a monocular camera. Systems that have been developed so far for the VI use either many cameras (stereo and monocular) integrated with other sensors or use very complex algorithms that are computationally expensive. In this research work, a computationally less expensive integrated system has been proposed to combine imaging geometry, Visual Odometry (VO), Object Detection (OD) along with Distance-Depth (D-D) estimation algorithms for precise navigation and localization by utilizing a single monocular camera as the only sensor. The developed algorithm is tested for both standard Karlsruhe and indoor environment recorded datasets. Tests have been carried out in real-time using a smartphone camera that captures image data of the environment as the person moves and is sent over Wi-Fi for further processing to the MATLAB software model running on an Intel i7 processor. The algorithm provides accurate results on real-time navigation in the environment with an audio feedback about the person's location. The trajectory of the navigation is expressed in an arbitrary scale. Object detection based localization is accurate. The D-D estimation provides distance and depth measurements up to an accuracy of 94-98{\%}.},
author = {Ramesh, Kruthika and Nagananda, S. N. and Ramasangu, Hariharan and Deshpande, Rohini},
doi = {10.1109/IEA.2018.8387082},
file = {:home/bruno/Documentos/Mestrado/08387082.pdf:pdf},
isbn = {9781538657478},
journal = {2018 5th International Conference on Industrial Engineering and Applications, ICIEA 2018},
keywords = {KLT tracker,image processing,monocular camera,optical flow,pattern recognition,visual SLAM,visual odometry},
pages = {122--128},
publisher = {IEEE},
title = {{Real-time localization and navigation in an indoor environment using monocular camera for visually impaired}},
year = {2018}
}
@article{Huang2019,
abstract = {Advanced driver assistance systems (ADAS) based on monocular vision are rapidly becoming a popular research subject. In ADAS, inter-vehicle distance estimation from an in-car camera based on monocular vision is critical. At present, related methods based on a monocular vision for measuring the absolute distance of vehicles ahead experience accuracy problems in terms of the ranging result, which is low, and the deviation of the ranging result between different types of vehicles, which is large and easily affected by a change in the attitude angle. To improve the robustness of a distance estimation system, an improved method for estimating the distance of a monocular vision vehicle based on the detection and segmentation of the target vehicle is proposed in this paper to address the vehicle attitude angle problem. The angle regression model (ARN) is used to obtain the attitude angle information of the target vehicle. The dimension estimation network determines the actual dimensions of the target vehicle. Then, a 2D base vector geometric model is designed in accordance with the image analytic geometric principle to accurately recover the back area of the target vehicle. Lastly, area-distance modeling based on the principle of camera projection is performed to estimate distance. The experimental results on the real-world computer vision benchmark, KITTI, indicate that our approach achieves superior performance compared with other existing published methods for different types of vehicles (including front and sideway vehicles).},
author = {Huang, Liqin and Zhe, Ting and Wu, Junyi and Wu, Qiang and Pei, Chenhao and Chen, Dan},
doi = {10.1109/ACCESS.2019.2907984},
file = {:home/bruno/Documentos/Mestrado/08678911.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Attitude angle information,distance estimation,instance segmentation,monocular vision},
pages = {46059--46070},
publisher = {IEEE},
title = {{Robust Inter-Vehicle Distance Estimation Method Based on Monocular Vision}},
volume = {7},
year = {2019}
}

@article{geiger2013vision,
  title={Vision meets robotics: The kitti dataset},
  author={Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1231--1237},
  year={2013},
  publisher={Sage Publications Sage UK: London, England}
}


@inproceedings{caesar2020nuscenes,
  title={nuscenes: A multimodal dataset for autonomous driving},
  author={Caesar, Holger and Bankiti, Varun and Lang, Alex H and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11621--11631},
  year={2020}
}


@article{Badue2019,
abstract = {We survey research on self-driving cars published in the literature focusing on autonomous cars developed since the DARPA challenges, which are equipped with an autonomy system that can be categorized as SAE level 3 or higher. The architecture of the autonomy system of self-driving cars is typically organized into the perception system and the decision-making system. The perception system is generally divided into many subsystems responsible for tasks such as self-driving-car localization, static obstacles mapping, moving obstacles detection and tracking, road mapping, traffic signalization detection and recognition, among others. The decision-making system is commonly partitioned as well into many subsystems responsible for tasks such as route planning, path planning, behavior selection, motion planning, and control. In this survey, we present the typical architecture of the autonomy system of self-driving cars. We also review research on relevant methods for perception and decision making. Furthermore, we present a detailed description of the architecture of the autonomy system of the self-driving car developed at the Universidade Federal do Esp$\backslash$'irito Santo (UFES), named Intelligent Autonomous Robotics Automobile (IARA). Finally, we list prominent self-driving car research platforms developed by academia and technology companies, and reported in the media.},
archivePrefix = {arXiv},
arxivId = {1901.04407},
author = {Badue, Claudine and Guidolini, R{\^{a}}nik and Carneiro, Raphael Vivacqua and Azevedo, Pedro and Cardoso, Vinicius Brito and Forechi, Avelino and Jesus, Luan and Berriel, Rodrigo and Paix{\~{a}}o, Thiago and Mutz, Filipe and Veronese, Lucas and Oliveira-Santos, Thiago and {De Souza}, Alberto Ferreira},
eprint = {1901.04407},
file = {:home/bruno/Downloads/1901.04407.pdf:pdf},
keywords = {behavior selection,detection,motion,moving,moving objects detection,objects tracking,obstacle avoidance,occupancy grid mapping,planning,recognition,road mapping,robot control,robot localization,route planning,self-driving cars,tra ffi c signalization},
title = {{Self-Driving Cars: A Survey}},
url = {http://arxiv.org/abs/1901.04407},
year = {2019}
}
@inproceedings{JustinoGarciaPraciano2019,
abstract = {Text classification techniques and sentiment analysis can be applied to understand and predict the behavior of users by exploiting the massive amount of data available on social networks. In this context, trend analysis tools based on supervised machine learning are crucial. In this work, a framework for spatiooral trend analysis of Brazilian presidential election trends based on Twitter data is proposed. Experimental results show that the proposed framework presents good effectiveness in predicting election results as well as providing tweet author's geolocation and tweet timestamp. According to our results the spatio trend analysis applying our framework via SVM on the Twitter data returns an accuracy close to 90{\%} when the Support Vector Machine (SVM) algortihm is applied for sentiment classification.},
author = {{Justino Garcia Praciano}, Bruno and {Carvalho Lustosa Da Costa}, Joao Paulo and {Abreu Maranhao}, Joao Paulo and {Lopes De Mendonca}, Fabio Lucio and {De Sousa Junior}, Rafael Timoteo and {Barbosa Prettz}, Juliano},
booktitle = {IEEE International Conference on Data Mining Workshops, ICDMW},
doi = {10.1109/ICDMW.2018.00192},
isbn = {9781538692882},
issn = {23759259},
keywords = {Big Data,Supervised Machine Learning,Support Vector Machine,Trend Analysis},
pages = {1355--1360},
title = {{Spatiooral trend analysis of the brazilian elections based on twitter data}},
volume = {2018-Novem},
year = {2019}
}
@article{No2001,
abstract = {Inhibitor for delta5 desaturase},
author = {No, Patent},
file = {:home/bruno/Documentos/Mestrado/US10192312.pdf:pdf},
journal = {Young},
number = {12},
pages = {0--2},
title = {{States Patent}},
volume = {1},
year = {2001}
}
@article{Lategahn2013,
abstract = {Next generation driver assistance systems require precise self localization. Common approaches using global navigation satellite systems (GNSSs) suffer from multipath and shadowing effects often rendering this solution insufficient. In urban environments this problem becomes even more pronounced. Herein we present a system for six degrees of freedom (DOF) ego localization using a mono camera and an inertial measurement unit (IMU). The camera image is processed to yield a rough position estimate using a previously computed landmark map. Thereafter IMU measurements are fused with the position estimate for a refined localization update. Moreover, we present the mapping pipeline required for the creation of landmark maps. Finally, we present experiments on real world data. The accuracy of the system is evaluated by computing two independent ego positions of the same trajectory from two distinct cameras and investigating these estimates for consistency. A mean localization accuracy of 10 cm is achieved on a 10 km sequence in an inner city scenario. {\textcopyright} 2013 IEEE.},
author = {Lategahn, Henning and Schreiber, Markus and Ziegler, Julius and Stiller, Christoph},
doi = {10.1109/IVS.2013.6629552},
file = {:home/bruno/Documentos/Mestrado/LategahnSchreiberZieglerStiller2013iv.pdf:pdf},
isbn = {9781467327558},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {June},
pages = {719--724},
title = {{Urban localization with camera and inertial measurement unit}},
year = {2013}
}
@article{Bao2016,
abstract = {Self-localization is one of the most important part in autonomous driving system. In urban canyon, the multipath and non-line-of-sight effects to GPS receiver decrease the precision of self-localization of the vehicle. More specifically, the lateral error is more serious because of the blockage of the satellites. However, the building on roadside could be the stable reference object for localization. Therefore, this paper proposes to use stereo camera and 3D building map to reduce the lateral error of positioning result. In our proposal, stereo camera is used to detect and reconstruct the building side view. Lateral distance between building and vehicle estimated by stereo camera is compared with 3D building map to rectify the lateral position of vehicle. In addition, this paper employs inertial sensor and GPS receiver to decide the longitudinal position of vehicle. The particle filter is used for the sensor fusion. The experiment is conducted in the center of Tokyo, Japan, which is a typical urban city scene with high density of tall buildings. It demonstrates that the proposed method could achieve sub-meter level accuracy in GPS difficult environments.},
author = {Bao, Jiali and Gu, Yanlei and Hsu, Li Ta and Kamijo, Shunsuke},
doi = {10.1109/IVS.2016.7535499},
file = {:home/bruno/Documentos/Mestrado/07535499.pdf:pdf},
isbn = {9781509018215},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
number = {June},
pages = {927--932},
title = {{Vehicle self-localization using 3D building map and stereo camera}},
volume = {2016-Augus},
year = {2016}
}
@article{Tram2018,
abstract = {This paper proposes an intelligent transport system positioning technique that determines the distance between vehicles via image sensor-based visible light communication. The proposed algorithm uses two image sensors and requires only one LED to estimate the distance. Furthermore, the cameras installed in vehicles are often of an inferior resolution that is insufficient to accurately determine the coordinates of the pixels on the image sensor. This paper addresses this problem by proposing a method that accurately determines the distance between two vehicles when the camera resolution is low. Simulations are conducted to verify the performance of the proposed algorithm.},
author = {Tram, Vo Thi Bich and Yoo, Myungsik},
doi = {10.1109/ACCESS.2018.2793306},
file = {:home/bruno/Documentos/Mestrado/08259282.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Distance estimation,image sensor,resolution,vehicle,visible light communication},
pages = {4521--4527},
publisher = {IEEE},
title = {{Vehicle-To-Vehicle Distance Estimation Using a Low-Resolution Camera Based on Visible Light Communications}},
volume = {6},
year = {2018}
}
@ARTICLE{8678911,
  author={L. {Huang} and T. {Zhe} and J. {Wu} and Q. {Wu} and C. {Pei} and D. {Chen}},
  journal={IEEE Access}, 
  title={Robust Inter-Vehicle Distance Estimation Method Based on Monocular Vision}, 
  year={2019},
  volume={7},
  number={},
  pages={46059-46070},}
  
  @INPROCEEDINGS{7759904,
  author={A. A. {Ali} and H. A. {Hussein}},
  booktitle={2016 Al-Sadeq International Conference on Multidisciplinary in IT and Communication Science and Applications (AIC-MITCSA)}, 
  title={Distance estimation and vehicle position detection based on monocular camera}, 
  year={2016},
  volume={},
  number={},
  pages={1-4},}