\chapter{State of the art} \label{capitulo2}

The authors from the paper \cite{Lategahn2013} have presented the next generation driver assistance systems require precise self localization. A new  system for six degrees of freedom ego localization using a mono camera and an inertial measurement unit. The accuracy of the system is evaluated by computing two independent ego positions of the same trajectory from two distinct cameras and investigating these estimates for consistency.

The authors of  \cite{Sankaranarayanan2008} said the video cameras are among the most commonly used sensors in a large number of applications, ranging from surveillance to smart rooms for videoconferencing. In this regard, the main focus of this paper is towards highlighting the efficient use of the geometric constraints induced by the imaging devices to derive distributed algorithms for target detection, tracking, and recognition.


In \cite{Unlu2019} the authors said that the drone has seen a tremendous increase in last few years, making these devices highly accessible to public. And Computer vision is extensively used to detect drones autonomously compared to other proposed solutions such as RADAR, acoustics and RF signal analysis thanks to its robustness. The authors proposed to combine multi-frame deep learning detection technique, where the frame coming from the zoomed camera on the turret is overlaid on the wide-angle static cameraâ€™s frame.


In paper \cite{Pei2019} was developed three-dimensional coordinates of objects captured by a sequence of images taken in different views, object reconstruction is a technique which aims to recover the shape and appearance information of objects.  A novel method to reconstruct occluded objects based on synthetic aperture imaging. The proposed method labels occlusion pixels according to variance and reconstructs the 3D point cloud based on synthetic aperture imaging.


In \cite{Zaarane2020}, the authors proposed an inter-vehicle distance measurement system for self-driving based on image processing. The proposed system uses two cameras mounted as one stereo camera, in the hosting vehicle behind the rear-view mirror. The results of the extensive experiments showed the high accuracy of the proposed method compared to the previous works from literature and it allows to measure efficiently the distances between the vehicles and the hosting vehicle. In addition, this method could be used in several systems of various domains in real time regardless of the object types.

This paper \cite{Wu2019} presented a multi-camera vehicle detection system that significantly improves the detection performance under occlusion conditions. They also inferred the vehicle position on the ground plane by leveraging multi-view cross-camera context.

In paper \cite{Ali2016} two approaches are defined: The  Estimation of the distance using an onboard camera and car position detection and the vehicle position detection is a method of specifying the relative position to the road that can serve as Lane Departure Warning system. For lane detection and tracking, Hough Transform and Kalman filter were adopted.


The authors of \cite{Hane2017} used a surround multi-camera system to cover the full 360-degree field-of-view around the car. Their pipeline is able to precisely calibrate multi-camera systems, build sparse 3D maps for visual navigation, visually localize the car with respect to these maps, generate accurate dense maps, as well as detect obstacles based on real-time depth map extraction.

Unlike existing methods which use pinhole cameras, the implementation of \cite{Cui2019} is based on fisheye cameras whose large field of view benefits various computer vision applications for self-driving vehicles such as visual-inertial odometry, visual localization, and object detection. To maintain both accuracy and efficiency, while accounting for the fact that fisheye images have a lower angular resolution, we recover the depths using multiple image resolutions. At the end of the pipeline, we fuse the fisheye depth images into the truncated signed distance function volume to obtain a 3D map.

In \cite{Huang2019}, inter-vehicle distance estimation from an in-car camera based on monocular vision is critical. To improve the robustness of a distance estimation system, an improved method for estimating the distance of a monocular vision vehicle based on the detection and segmentation of the target vehicle is proposed in this paper to address the vehicle attitude angle problem. The angle regression model is used to obtain the attitude angle information of the target vehicle. The dimension estimation network determines the actual dimensions of the target vehicle.

In \cite{Bao2016}, the multipath and non-line-of-sight effects to GPS receiver decrease the precision of self-localization of the vehicle. More specifically, the lateral error is more serious because of the blockage of the satellites. Lateral distance between building and vehicle estimated by stereo camera is compared with 3D building map to rectify the lateral position of vehicle. In addition, this paper employs inertial sensor and GPS receiver to decide the longitudinal position of vehicle.

In \cite{Tang2019} introduces CityFlow, a city-scale traffic camera dataset consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km. We expect this dataset to catalyze research in this field, propel the state-of-the-art forward, and lead to deployed traffic optimization in the real world.

In \cite{Qi2019} to measure the distance between our vehicle and the target vehicle by monocular vision and eliminate the estimation error bring by changing of vehicle pose, we propose the distance estimation method based on the vehicle pose information, which can be used to eliminate the error of distance estimation effectively cause by the change of the pitch angle and roll angle of unmanned vehicle. In addition, the pose information could also help us estimate whether the vehicle is in a slope, thus engage in distance estimation for the vehicles. Several groups of data was collected for the experiments.

In paper \cite{Wongsaree2018} presents a distance determination technique using an image from the single forward camera. Therefore, automatic brightness adjustment and inverse perspective mapping is applied in the proposed scheme. The experimental results confirm that the proposed technique can detect distance of the object in front of the car where the error is 7.96\%.

The paper \cite{Pan2019} the authors simulated experiments to verify the feasibility of the proposed method. Meanwhile, physical experiment results show this method can reduce the outdoor environment impact and improve the calibration and measurement precision effectively. And In paper \cite{Lin2014} presents a novel method of camera parameters calibration for obstacle distance measurement based on monocular vision and in the end, experiment shows that the proposed method is very effective.

In \cite{Simon2019a} the authors have performed  experiments on KITTI dataset for accurated 3D object detection and they achivied the same results as state-of-the-art in all related categories, while maintaining the performance and accuracy trade-off and still run in real-time.


In this paper, we propose weighted-mean YOLO to improve real-time performance of object detection by fusing information of RGB camera and LIDAR. We design the system using weighted-mean to construct a robust system and compared with other algorithms, it shows performance improvement of missed-detection \cite{Kim2019}. In paper \cite{Simon2019} introduced the Complex-YOLO, a state of the art real-time 3D object detection network on point clouds only. In this work, they described a network that expands YOLOv2, a fast 2D standard object detector for RGB images, by a specific complex regression strategy to estimate multi-class 3D boxes in Cartesian space. 


In \cite{Oliveira2015}, the method consists of mapping images to a new coordinate system where perspective effects are removed. The removal of perspective associated effects facilitates road and obstacle detection and also assists in free space estimation. As shown in the results, this considerably improves the effectiveness of the algorithm and reduces computation time when compared with the classical inverse perspective mapping.

The approach introduced by \cite{Salman2017} a stereo camera was used and the calculation of distance considers angular distance, distance between cameras, and the pixel of the image. They proposed a method that measures object distance based on trigonometry.   

 In \cite{Rangesh2019} a new framework for tracking multiple objects  is presented. They used fusion techniques to achieve this. Second, the objects of interest are tracked directly in the real world. They tested it on real world highway data collected from a heavily sensorized testbed capable of capturing full-surround information.



This paper proposes an intelligent transport system positioning technique that determines the distance between vehicles via image sensor-based visible light communication. This problem by proposing a method that accurately determines the distance between two vehicles when the camera resolution is low \cite{Tram2018}.


The object detection, classification and localization in the real world scenario is investigated and discussed by \cite{Hofmann2019}. And with the contrast, the proposed approach fuses three different algorithms for object detection and classification and uses stereo vision for object localization. The combination of these complementary object detection principles allows the reliable detection of dynamic as well as static objects. An algorithm for fusing the results of the three object detection methods based on bounding boxes is introduced. The proposed fusion algorithm for bounding boxes improves the detection results and provides an information fusion.

